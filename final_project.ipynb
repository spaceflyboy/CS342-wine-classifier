{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849a57cd",
   "metadata": {},
   "source": [
    "# CS 342 Neural Nets: Final Project\n",
    "\n",
    "Authors: Ryan Gahagan (rg32643) and Dustan Helm (dbh878)\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this project, we will create a neural network that will hopefully be able to predict the quality of various wines given their chemical compositions.\n",
    "\n",
    "This project's data set and idea are based off another paper, cited here:\n",
    "\n",
    "  \"P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "  \n",
    "  Modeling wine preferences by data mining from physicochemical properties.\n",
    "  \n",
    "  In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\"\n",
    "\n",
    "We plan to make a feed-forward network to process this data, as well as some experiments to test and analyze our network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51836a78",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "First, we have to take the data set and process it into a format that Python can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60be6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block to load important libraries and set things up\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb994c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in dataset files\n",
    "white_file = open(\"winequality-white-nolabels.csv\")\n",
    "wine_quality_white = np.loadtxt(white_file, delimiter=\";\")\n",
    "red_file = open(\"winequality-red-nolabels.csv\")\n",
    "wine_quality_red = np.loadtxt(red_file, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b4d724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine_quality_combined_whitered.shape = (6497, 12)\n"
     ]
    }
   ],
   "source": [
    "# Create suitable data_arrays\n",
    "num_samples_white = wine_quality_white.shape[0]\n",
    "num_samples_red = wine_quality_red.shape[0]\n",
    "num_samples_total = num_samples_white + num_samples_red\n",
    "\n",
    "# Combine white and red wine datasets into one np array\n",
    "wine_quality_combined_whitered = np.append(wine_quality_white, wine_quality_red, axis=0)\n",
    "\n",
    "print(f\"wine_quality_combined_whitered.shape = {wine_quality_combined_whitered.shape}\")\n",
    "\n",
    "assert wine_quality_combined_whitered.shape[0] == num_samples_total\n",
    "\n",
    "# Rename\n",
    "data_array = wine_quality_combined_whitered\n",
    "data_array_white = wine_quality_white\n",
    "data_array_red = wine_quality_red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c0585",
   "metadata": {},
   "source": [
    "Now we've loaded our datasets into lists. `data_array_red` is an `np.array` whose shape is `(1599,12)` and `data_array_white` is likewise an array of shape `(4898,12)`. `data_array` is simply those two arrays concatenated into an array of shape `(6497,12)`.\n",
    "\n",
    "Note that these 12 columns represent both features and labels.\n",
    "\n",
    "The columns, in order, are:\n",
    "- fixed acidity\n",
    "- volatile acidity\n",
    "- citric acid\n",
    "- residual sugar\n",
    "- chlorides\n",
    "- free sulfur dioxide\n",
    "- total sulfur dioxide\n",
    "- density\n",
    "- pH\n",
    "- sulphates\n",
    "- alcohol\n",
    "- quality (integer in \\[0,10\\])\n",
    "where quality is our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd608f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data array into features and label arrays\n",
    "# Inputs: \n",
    "#     data_array: Data array to be split into features and labels\n",
    "# Outputs:\n",
    "#     data_array_feats: Features for data_array\n",
    "#     data_array_labels: Labels for data_array\n",
    "def split_features_labels(data_array):\n",
    "    assert isinstance(data_array, np.ndarray)\n",
    "    #assert data_array.shape[1] == 12\n",
    "    \n",
    "    data_array_feats = data_array[:,:-1] # first 11 columns\n",
    "    data_array_labels = data_array[:,-1] # last column\n",
    "\n",
    "    #assert data_array_feats.shape[1] == 11\n",
    "    assert data_array_feats.shape[0] == data_array_labels.shape[0]\n",
    "    \n",
    "    return data_array_feats, data_array_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e892a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data array into training and testing sets based on the provided train_proportion parameter\n",
    "# Inputs: \n",
    "#     data_array: Dataset to split for training and testing\n",
    "#     train_proportion: Proportion of datapoints to be kept as training data\n",
    "# Outputs:\n",
    "#     train_set: Training set containing a proportion of the datapoints contained in data_array specified by input parameter.\n",
    "#     test_set: Testing set containing held-back datapoints to test the trained model\n",
    "def train_test_split(data_array, train_proportion):\n",
    "    assert isinstance(data_array, np.ndarray)\n",
    "    #assert data_array.shape[1] == 12\n",
    "    \n",
    "    num_samples = data_array.shape[0]\n",
    "    \n",
    "    feats, labels = split_features_labels(data_array)\n",
    "    data_set = torch.utils.data.TensorDataset(torch.tensor(feats), torch.tensor(labels).long())\n",
    "    \n",
    "    train_size = int(train_proportion*num_samples)\n",
    "    test_size = num_samples - train_size\n",
    "    \n",
    "    train_set, test_set = torch.utils.data.random_split(data_set, [train_size, test_size])\n",
    "    \n",
    "    assert abs(len(train_set) / len(data_array) - train_proportion) < 0.01\n",
    "    assert abs(len(test_set) / len(data_array)  - (1 - train_proportion)) < 0.01\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a870039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the actual split on all of our datasets into training and testing\n",
    "train_proportion = 0.8\n",
    "train_proportion_white = 0.8\n",
    "train_proportion_red = 0.8\n",
    "\n",
    "train_set, test_set = train_test_split(data_array, train_proportion)\n",
    "train_set_white, test_set_white = train_test_split(data_array_white, train_proportion_white)\n",
    "train_set_red, test_set_red = train_test_split(data_array_red, train_proportion_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b35420ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set into true training and validation data based on the input proportion\n",
    "# Inputs:\n",
    "#     ntotal: Total number of datapoints in the original training set to be used to determine the split\n",
    "#     train_proportion: Proportion of the training set examples which should not be placed into the validation set\n",
    "# Outputs:\n",
    "#     train_ix: Indices for training examples\n",
    "#     val_ix: Indices for validation examples\n",
    "\n",
    "def train_val_split_ix(ntotal, train_proportion):\n",
    "    ntrain = int(train_proportion*ntotal)\n",
    "    nval = ntotal - ntrain\n",
    "    \n",
    "    val_ix = np.random.choice(range(ntotal), size=nval, replace=False)\n",
    "    train_ix = list(set(range(ntotal)) - set(val_ix))\n",
    "    \n",
    "    assert abs(len(train_ix) / ntotal - train_proportion) < 0.01\n",
    "    assert abs(len(val_ix) / ntotal - (1 - train_proportion)) < 0.01\n",
    "    \n",
    "    return (train_ix, val_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb14a88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(len(train_ix), len(val_ix)) = (4677, 520)\n",
      "(len(train_white_ix), len(val_white_ix)) = (3526, 392)\n",
      "(len(train_red_ix), len(val_red_ix)) = (1151, 128)\n"
     ]
    }
   ],
   "source": [
    "# Perform the training/validation split and then confirm array lengths\n",
    "train_proportion2 = 0.9\n",
    "train_proportion_white2 = 0.9\n",
    "train_proportion_red2 = 0.9\n",
    "\n",
    "train_ix, val_ix = train_val_split_ix(len(train_set), train_proportion2)\n",
    "train_white_ix, val_white_ix = train_val_split_ix(len(train_set_white), train_proportion_white2)\n",
    "train_red_ix, val_red_ix = train_val_split_ix(len(train_set_red), train_proportion_red2)\n",
    "\n",
    "print(f\"(len(train_ix), len(val_ix)) = ({len(train_ix)}, {len(val_ix)})\")\n",
    "print(f\"(len(train_white_ix), len(val_white_ix)) = ({len(train_white_ix)}, {len(val_white_ix)})\")\n",
    "print(f\"(len(train_red_ix), len(val_red_ix)) = ({len(train_red_ix)}, {len(val_red_ix)})\")\n",
    "\n",
    "assert len(train_ix) + len(val_ix) == len(train_set)\n",
    "assert len(train_white_ix) + len(val_white_ix) == len(train_set_white)\n",
    "assert len(train_red_ix) + len(val_red_ix) == len(train_set_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e44ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data samplers for use in DataLoader objects\n",
    "# Inputs: \n",
    "#     datalist_ix: Tuple of index lists used to determine each loader's data\n",
    "# Outputs:\n",
    "#     result: Tuple of SubsetRandomSamplers representing each index list object in datalist_ix\n",
    "def setup_samplers(datalist_ix):\n",
    "    result = ()\n",
    "    for data_ix in datalist_ix:\n",
    "        result += (SubsetRandomSampler(data_ix),)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adf04a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a tuple of Data Loaders based on provided datasets, samplers, and batch_size\n",
    "# Inputs:\n",
    "#     datalist: List of datasets to give to the DataLoaders\n",
    "#     samplers: List of samplers to use in the DataLoaders\n",
    "#     batch_size: DataLoader batch size (Currently uses the same batch_size for every dataset passed)\n",
    "# Outputs:\n",
    "#     Tuple of DataLoader objects len(datalist) size long \n",
    "def setup_data_loaders(datalist, samplers, batch_size):\n",
    "    assert len(datalist) == len(samplers)\n",
    "    \n",
    "    result = ()\n",
    "    for i in range(len(datalist)):\n",
    "        data = datalist[i]\n",
    "        sampler = samplers[i]\n",
    "        result += (torch.utils.data.DataLoader(data, batch_size, sampler=sampler),)\n",
    "    return result\n",
    "\n",
    "#TODO: It may be useful to have different batch_size values for training, validation, and testing. \n",
    "# In that event, batch_size should be replaced with a touple of batch_size values and the following should be added to the loop:\n",
    "# batch_size = batch_sizes[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f05333",
   "metadata": {},
   "source": [
    "Now that we've declared methods to do various helper tasks, we're going to actually break our data into useful information.\n",
    "\n",
    "The three sets of data (red, white, and combined) will be each partitioned into a training set, a validation set, and a testing set (whose sizes will be proportional to the variables declared above). We will then create `DataLoader` objects for each of these partitions so that we can iterate over them in our training section.\n",
    "\n",
    "Note here that we also declare batch sizes to determine how many pieces of information are trained on at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01ca66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up samplers and DataLoaders for all three datasets (Combined, White, Red)\n",
    "batch_size = 100\n",
    "batch_size_white = 100\n",
    "batch_size_red = 100\n",
    "\n",
    "#COMBINED DATASET\n",
    "sampler_input = (train_ix, val_ix)\n",
    "train_sampler, val_sampler = setup_samplers(sampler_input)\n",
    "\n",
    "datalist = (train_set, train_set, test_set)\n",
    "samplers = (train_sampler, val_sampler, None)\n",
    "train_loader, val_loader, test_loader = setup_data_loaders(datalist, samplers, batch_size)\n",
    "\n",
    "#JUST WHITE\n",
    "sampler_input_white = (train_white_ix, val_white_ix)\n",
    "train_sampler_white, val_sampler_white = setup_samplers(sampler_input_white)\n",
    "\n",
    "datalist_white = (train_set_white, train_set_white, test_set_white)\n",
    "samplers_white = (train_sampler_white, val_sampler_white, None)\n",
    "train_loader_white, val_loader_white, test_loader_white = setup_data_loaders(datalist_white, samplers_white, batch_size_white)\n",
    "\n",
    "#JUST RED\n",
    "sampler_input_red = (train_red_ix, val_red_ix)\n",
    "train_sampler_red, val_sampler_red = setup_samplers(sampler_input_red)\n",
    "\n",
    "datalist_red = (train_set_red, train_set_red, test_set_red)\n",
    "samplers_red = (train_sampler_red, val_sampler_red, None)\n",
    "train_loader_red, val_loader_red, test_loader_red = setup_data_loaders(datalist_red, samplers_red, batch_size_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e47e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-splits the data into new random chunks to help reduce noise\n",
    "#\n",
    "# Inputs: data_arrays for combined data set, white wine dataset, and red dataset to generate loaders from\n",
    "# Outputs:\n",
    "#    A list [full, white, red] of loader tuples (train, val, test)\n",
    "def get_random_loaders(data_array, data_array_white, data_array_red):\n",
    "    batch_size = 100\n",
    "    batch_size_white = 100\n",
    "    batch_size_red = 100\n",
    "    \n",
    "    # Get train/test sets\n",
    "    train_set, test_set = train_test_split(data_array, train_proportion)\n",
    "    train_set_white, test_set_white = train_test_split(data_array_white, train_proportion_white)\n",
    "    train_set_red, test_set_red = train_test_split(data_array_red, train_proportion_red)\n",
    "    \n",
    "    # Get train/val indices\n",
    "    train_ix, val_ix = train_val_split_ix(len(train_set), train_proportion2)\n",
    "    train_white_ix, val_white_ix = train_val_split_ix(len(train_set_white), train_proportion_white2)\n",
    "    train_red_ix, val_red_ix = train_val_split_ix(len(train_set_red), train_proportion_red2)\n",
    "    \n",
    "    #COMBINED DATASET\n",
    "    sampler_input = (train_ix, val_ix)\n",
    "    train_sampler, val_sampler = setup_samplers(sampler_input)\n",
    "\n",
    "    datalist = (train_set, train_set, test_set)\n",
    "    samplers = (train_sampler, val_sampler, None)\n",
    "    train_loader, val_loader, test_loader = setup_data_loaders(datalist, samplers, batch_size)\n",
    "\n",
    "    #JUST WHITE\n",
    "    sampler_input_white = (train_white_ix, val_white_ix)\n",
    "    train_sampler_white, val_sampler_white = setup_samplers(sampler_input_white)\n",
    "\n",
    "    datalist_white = (train_set_white, train_set_white, test_set_white)\n",
    "    samplers_white = (train_sampler_white, val_sampler_white, None)\n",
    "    train_loader_white, val_loader_white, test_loader_white = setup_data_loaders(datalist_white, samplers_white, batch_size_white)\n",
    "\n",
    "    #JUST RED\n",
    "    sampler_input_red = (train_red_ix, val_red_ix)\n",
    "    train_sampler_red, val_sampler_red = setup_samplers(sampler_input_red)\n",
    "\n",
    "    datalist_red = (train_set_red, train_set_red, test_set_red)\n",
    "    samplers_red = (train_sampler_red, val_sampler_red, None)\n",
    "    train_loader_red, val_loader_red, test_loader_red = setup_data_loaders(datalist_red, samplers_red, batch_size_red)\n",
    "    \n",
    "    return [(train_loader, val_loader, test_loader), \\\n",
    "           (train_loader_white, val_loader_white, test_loader_white), \\\n",
    "           (train_loader_red, val_loader_red, test_loader_red)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96077d94",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "With our data processed and ready to be used, now we will write some functions to train and test a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fd3f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the provided model with data gathered from train_loader, the given criterion, and the given optimizer.\n",
    "# Additionally perform validation checks with data from val_loader.\n",
    "# Inputs:\n",
    "#     model: Neural network to train\n",
    "#     train_loader: DataLoader which provides training data to the model \n",
    "#     val_loader: DataLoader which provides validation data to the model\n",
    "#     criterion: Loss Function which trains the model\n",
    "#     optimizer: Optimization algorithm to improve loss during training \n",
    "#     nepoch: Number of epochs to train for (Defaults to 100)\n",
    "# Outputs:\n",
    "#     Prints the Training and Validation loss at each epoch\n",
    "def train_network(model, train_loader, val_loader, criterion, optimizer, nepoch=100,silent=True):\n",
    "    try:\n",
    "        cur_range = None\n",
    "        if silent:\n",
    "            cur_range = range(nepoch)\n",
    "        else:\n",
    "            cur_range = tqdm(range(nepoch))\n",
    "\n",
    "        for epoch in cur_range:\n",
    "            # Train over each epoch with a progress bar (tqdm)\n",
    "            if not silent: \n",
    "                print('EPOCH %d'%epoch)\n",
    "            \n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for inputs, labels in train_loader:\n",
    "                # For each train input: \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward propagate inputs\n",
    "                outputs = model.forward(inputs)\n",
    "                \n",
    "                # Compute loss and learn\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Add current loss to batch average\n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "            \n",
    "            # Show Training loss for current epoch over the train_loader data\n",
    "            if not silent:\n",
    "                print('{:>12s} {:>7.5f}'.format('Train loss:', total_loss/count))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Perform Validation checks on the newly trained model\n",
    "                total_loss = 0\n",
    "                count = 0\n",
    "                for inputs, labels in val_loader:\n",
    "                    # Forward propagate inputs\n",
    "                    outputs = model.forward(inputs)\n",
    "                    \n",
    "                    # Compute Loss\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Add current loss to batch average\n",
    "                    total_loss += loss.item()\n",
    "                    count += 1\n",
    "                    \n",
    "                # Show Validation loss for current epoch over the val_loader data\n",
    "                if not silent:\n",
    "                    print('{:>12s} {:>7.5f}'.format('Val loss:', total_loss/count))\n",
    "                    print()\n",
    "    except KeyboardInterrupt as e:\n",
    "        print('Exiting from training early')\n",
    "        raise e\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c21a1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the provided model with data from test_loader\n",
    "# Inputs:\n",
    "#     model: Model to test using unseen data\n",
    "#     test_loader: DataLoader to provide held-back testing data to trained model\n",
    "#     mode: String used at front of each loss printout\n",
    "# Outputs:\n",
    "#     acc: Top-1 accuracy of the model on the testing data in percent\n",
    "#     true: Array of actual labels\n",
    "#     pred: Array of model-predicted labels\n",
    "def test_network(model, test_loader, mode=\"test_network\", silent=True):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true, pred = [], []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, labels  in test_loader:\n",
    "            # Forward propagate testing data\n",
    "            outputs = model.forward(inputs)\n",
    "            \n",
    "            # Get the prediction for inputs\n",
    "            vals, predicted = torch.max(outputs, dim=1) \n",
    "            \n",
    "            # Tally results \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            true.append(labels)\n",
    "            pred.append(predicted)\n",
    "    \n",
    "    # Compute and print final accuracy, then format outputs\n",
    "    acc = (100 * correct / total)\n",
    "    if not silent:\n",
    "        print('%s accuracy: %0.3f' % (mode, acc))\n",
    "    true = np.concatenate(true)\n",
    "    pred = np.concatenate(pred)\n",
    "    return acc, true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b5ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single method to wrap all of training and testing\n",
    "#\n",
    "# Inputs:\n",
    "#   Positional:\n",
    "#     model: the model to train/test (should be newly initialized)\n",
    "#     train_loader: DataLoader for training set\n",
    "#     val_loader: DataLoader for validation set\n",
    "#     test_loader: DataLoader for testing set\n",
    "#     criterion: loss function (e.g. CrossEntropyLoss)\n",
    "#     optimizer: optimizing function (e.g. Adam)\n",
    "#   kwargs (keyword):\n",
    "#     nepoch: number of epochs, defaults to 100\n",
    "#     mode: string to print during test, defaults to \"Model\"\n",
    "#     train_silent: boolean to silence training, defaults to True\n",
    "#     test_silent: boolean to silence testing, defaults to True\n",
    "# Outputs:\n",
    "#     acc: accuracy of testing, in range (0, 100)\n",
    "#     true: list of true labels\n",
    "#     pred: list of predicted labels\n",
    "def train_and_test(model, train_loader, val_loader, test_loader, criterion, \\\n",
    "                   optimizer, **kwargs):\n",
    "    # Get keyword args, and default missing\n",
    "    nepoch = kwargs.get(\"nepoch\", 100)\n",
    "    mode = kwargs.get(\"mode\", \"Model\")\n",
    "    train_silent = kwargs.get(\"train_silent\", True)\n",
    "    test_silent = kwargs.get(\"test_silent\", True)\n",
    "    try:\n",
    "        train_network(model, train_loader, val_loader, criterion, optimizer, nepoch, train_silent)\n",
    "    except KeyboardInterrupt as e:\n",
    "        print(\"train_and_test: Keyboard Interrupt detected, raising an exception\")\n",
    "        raise e\n",
    "    model.eval()\n",
    "    acc, true, pred = test_network(model, test_loader, mode, test_silent)\n",
    "    return acc, true, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cc903",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "With our training and testing functionality equipped, now we will actually decide how to build our model.\n",
    "\n",
    "Note here that there will be a heavy focus on making the model flexible so that we can tune hyperparameters or test new input varieties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325ac4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineQualityModel(torch.nn.Module):\n",
    "    # Constructor for a WineQualityModel\n",
    "    # Inputs:\n",
    "    #     layers: a tuple of layers that you want in the model\n",
    "    #       note that the output shape must be of length 10\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # NOTE: this gives the construction tons of flexibility\n",
    "        # but also leaves plenty of room for dimensionality errors\n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers.forward(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3df1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WineQualityModel((\n",
    "    torch.nn.Linear(11, 81),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(81, 11)\n",
    "))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_and_test(model, train_loader, val_loader, test_loader, criterion, \\\n",
    "               optimizer, mode=\"Combined Model\", test_silent=False)\n",
    "\n",
    "model_white = WineQualityModel((\n",
    "    torch.nn.Linear(11, 81),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(81, 11)\n",
    "))\n",
    "\n",
    "optimizer_white = torch.optim.Adam(model_white.parameters(), lr=1e-3)\n",
    "train_and_test(model_white, train_loader_white, val_loader_white, test_loader_white, \\\n",
    "               criterion, optimizer_white, mode=\"White Model\", test_silent=False)\n",
    "\n",
    "model_red = WineQualityModel((\n",
    "    torch.nn.Linear(11, 81),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(81, 11)\n",
    "))\n",
    "\n",
    "optimizer_red = torch.optim.Adam(model_red.parameters(), lr=1e-3)\n",
    "_, _, _ = train_and_test(model_red, train_loader_red, val_loader_red, test_loader_red, \\\n",
    "                         criterion, optimizer_red, mode=\"Red Model\", test_silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b295cf7",
   "metadata": {},
   "source": [
    "## Hypertuning\n",
    "\n",
    "Now that we have the ability to initialize and train a model, we're going to try hypertuning some of the parameters. Specifically we want to try tuning the number of layers and the hidden size of each of those layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14b4ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a set of layers, creates a model that fits that specification\n",
    "# then trains and tests it, all in one method. There are actually three\n",
    "# models of identical specification constructed and trained separately,\n",
    "# and their results are likewise returned piecewise.\n",
    "#\n",
    "# Inputs:\n",
    "#     layers: a tuple of all the layers you want in your model\n",
    "# Outputs:\n",
    "#     true_lists: a tuple of lists of all the actual labels per dataset\n",
    "#                 in the order (full, white, red)\n",
    "#     pred_lists: a tuple of lists of model predictions per model/dataset\n",
    "#                 in the order (full, white, red)\n",
    "def create_and_report_model(layers):\n",
    "    assert len(layers) > 0\n",
    "    \n",
    "    # Check that you start and end with Linears\n",
    "    assert isinstance(layers[0], torch.nn.Linear)\n",
    "    assert isinstance(layers[-1], torch.nn.Linear)\n",
    "    \n",
    "    # Check that first layer takes in 11\n",
    "    # we may need to remove this during computational experiments\n",
    "    assert layers[0].weight.shape[1] == 11\n",
    "    \n",
    "    # Check that final layer outputs 11\n",
    "    assert layers[-1].weight.shape[0] == 11\n",
    "    \n",
    "    loaders = get_random_loaders(data_array, data_array_white, data_array_red)\n",
    "\n",
    "    true_lists = []\n",
    "    pred_lists = []\n",
    "    for i in range(3):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        cur_loaders = loaders[i]\n",
    "        cur_model = WineQualityModel(layers)\n",
    "        optimizer = torch.optim.Adam(cur_model.parameters(), lr=1e-3)\n",
    "        try:\n",
    "            acc, true, pred = train_and_test(cur_model, *cur_loaders, criterion, \\\n",
    "                                         optimizer, nepoch=250, mode=f\"Model {i}\")\n",
    "        except KeyboardInterrupt as e:\n",
    "            print(\"create_and_report_model. Training ended early. Raising an exception\")\n",
    "            raise e\n",
    "        \n",
    "        true_lists.append(true)\n",
    "        pred_lists.append(pred)\n",
    "\n",
    "    return true_lists, pred_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44f098d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the accuracy of a prediction list compared to a true list\n",
    "# Measures two accuracies: exact accuracy, and tolerance 1 accuracy\n",
    "#\n",
    "# Inputs:\n",
    "#     true: a list of actual labels\n",
    "#     pred: a list of predicted labels\n",
    "# Outputs:\n",
    "#     exact_acc: percentage (in range (0,100)) of exact matches\n",
    "#     t1_acc: percentage (range of (0,100)) of 1-tolerance matches\n",
    "def compute_model_accuracies(true, pred):\n",
    "    assert len(true) == len(pred)\n",
    "    \n",
    "    num_exact = 0\n",
    "    num_1_tolerant = 0\n",
    "    \n",
    "    for i in range(len(true)):\n",
    "        t = true[i]\n",
    "        p = pred[i]\n",
    "        \n",
    "        if t == p:\n",
    "            # Exact match\n",
    "            num_exact += 1\n",
    "\n",
    "        if abs(t - p) <= 1:\n",
    "            # Within one (e.g. 6,7,8 are all within 1 of 7)\n",
    "            num_1_tolerant += 1\n",
    "    \n",
    "    exact_acc = num_exact / len(true)\n",
    "    t1_acc = num_1_tolerant / len(true)\n",
    "    return 100 * exact_acc, 100 * t1_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f14b110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full accuracies (exact, T1):\n",
      "(51.61538461538462, 94.84615384615384)\n",
      "\n",
      "White accuracy:\n",
      "(54.18367346938775, 95.3061224489796)\n",
      "\n",
      "Red accuracy:\n",
      "(58.4375, 96.25)\n"
     ]
    }
   ],
   "source": [
    "# Example of how we might train everything\n",
    "true_lists, pred_lists = create_and_report_model((\n",
    "    torch.nn.Linear(11,49,bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.3),\n",
    "    torch.nn.Linear(49,11,bias=False),\n",
    "))\n",
    "print(\"Full accuracies (exact, T1):\")\n",
    "print(compute_model_accuracies(true_lists[0], pred_lists[0]))\n",
    "print(\"\\nWhite accuracy:\")\n",
    "print(compute_model_accuracies(true_lists[1], pred_lists[1]))\n",
    "print(\"\\nRed accuracy:\")\n",
    "print(compute_model_accuracies(true_lists[2], pred_lists[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0f9ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big ol' wrapper over the whole tuning test\n",
    "# Just pass in the layers you want to test, and it'll spit out\n",
    "# the accuracy of that tuning on all three datasets\n",
    "#\n",
    "# Inputs:\n",
    "#     layers: layers of your model (tuple)\n",
    "# Ouputs:\n",
    "#     accuracies: list of 3 tuples, where each tuple holds the accuracies\n",
    "#                 i.e. (exact_acc, t1_acc) as percentage from 0 to 100\n",
    "def test_tuning(layers):\n",
    "    num_noise_tests = 3\n",
    "    accuracies = [[0, 0], [0, 0], [0,0]]\n",
    "    for i in range(num_noise_tests):\n",
    "        try:\n",
    "            true_lists, pred_lists = create_and_report_model(layers)\n",
    "        except KeyboardInterrupt as e:\n",
    "            print(\"test_tuning: Training ended early due to keyboard interrupt. Raising an exception\")\n",
    "            raise e\n",
    "\n",
    "        for j in range(3):\n",
    "            cur_accs = compute_model_accuracies(true_lists[j], pred_lists[j])\n",
    "            accuracies[j][0] += cur_accs[0]\n",
    "            accuracies[j][1] += cur_accs[1]\n",
    "            \n",
    "    for i in range(len(accuracies)):\n",
    "        accuracies[i][0] /= num_noise_tests\n",
    "        accuracies[i][1] /= num_noise_tests\n",
    "        accuracies[i] = tuple(accuracies[i])\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "# The previous cell can now be written simply as:\n",
    "# accs = test_tuning((... layers))\n",
    "# You can use these accuracy tuples to print columns in a csv or smth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3cd6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns the accuracy list into a usable string\n",
    "# TODO: make thsi return your string in a way you actually\n",
    "# can use in a csv or whatever we want\n",
    "#\n",
    "# Inputs:\n",
    "#     accs: a list of tuple pairs representing the accuracies\n",
    "# Outputs:\n",
    "#     acc_str: a usable string representation of the accuracies\n",
    "#              cur format is [(full_exact, full_t1),(w_e,w_t),(r_e,r_t)]\n",
    "def get_accuracies_string(accs):\n",
    "    assert len(accs) == 3\n",
    "    assert len(accs[0]) == 2\n",
    "    assert len(accs[1]) == 2\n",
    "    assert len(accs[2]) == 2\n",
    "    \n",
    "    acc_str = \"[\"\n",
    "    for i in range(3):\n",
    "        acc_str += (\"(%0.2f,%0.2f)\" % (accs[i][0], accs[i][1]))\n",
    "        if i != 2:\n",
    "            acc_str += \",\"\n",
    "    return acc_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de90eadd",
   "metadata": {},
   "source": [
    "Now we've got plenty of methods to reduce the complexity of our hypertuning loops, so all that's left is to actually write then in a readable way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c271b3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive accuracies [(full_exact, full_t1), (white_exact, white_t1), (red_exact, red_t1)]:\n",
      "[(52.92,94.15),(52.45,94.18),(52.50,95.62)\n"
     ]
    }
   ],
   "source": [
    "# Here's a boring naive single layer test\n",
    "accs = test_tuning(\n",
    "    (torch.nn.Linear(11, 11, bias=False),) # super important comma\n",
    ")\n",
    "print(\"Naive accuracies [(full_exact, full_t1), (white_exact, white_t1), (red_exact, red_t1)]:\")\n",
    "print(get_accuracies_string(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7df4d32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(54.31,94.15),(53.27,95.00),(55.62,96.56)\n",
      "[(55.23,95.23),(55.10,95.41),(55.62,95.94)\n"
     ]
    }
   ],
   "source": [
    "accs = test_tuning((\n",
    "    torch.nn.Linear(11, 40, bias=True),\n",
    "    torch.nn.Dropout(0.3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(40, 11, bias=False),\n",
    "))\n",
    "print(get_accuracies_string(accs))\n",
    "\n",
    "accs = test_tuning((\n",
    "    torch.nn.Linear(11, 40, bias=True),\n",
    "    torch.nn.Dropout(0.3),\n",
    "    torch.nn.BatchNorm1d(40),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(40, 11, bias=False),\n",
    "))\n",
    "print(get_accuracies_string(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fc10042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get loopy\n",
    "\n",
    "# Builds a generic Linear->ReLU->Dropout->... layer chain\n",
    "#\n",
    "# Inputs:\n",
    "#     hiddens: a list of all hidden sizes in order (not empty)\n",
    "#     input_features (default = 11): integer number of input features to give the model\n",
    "# Outputs:\n",
    "#     layers: the generic layers\n",
    "def build_generic_layers(hiddens, input_features=11):\n",
    "    full = [input_features] + hiddens + [11]\n",
    "    stop = len(full) - 1\n",
    "    layers = tuple()\n",
    "    for i in range(stop):\n",
    "        if i != (stop - 1):\n",
    "            layers += (\n",
    "                torch.nn.Linear(full[i], full[i+1], bias=True),\n",
    "                torch.nn.Dropout(0.25),\n",
    "                torch.nn.BatchNorm1d(full[i+1]),\n",
    "                torch.nn.ReLU(),\n",
    "            )\n",
    "        else:\n",
    "            layers += (\n",
    "                torch.nn.Linear(full[i], full[i+1], bias=False),\n",
    "            )\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa18b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests a generic n-layer network, given the number of hidden\n",
    "# layers and the potential sizes of every hidden layer\n",
    "#\n",
    "# Inputs:\n",
    "#     n: number of hidden layers (> 0)\n",
    "#     hidden_sizes: tuple of lists of hidden sizes (one list per layer)\n",
    "#       ex: ([1,2,3], [5,6,7]) would be two layers with three sizes each\n",
    "#     silent (default = True): Determines whether debug printouts will run\n",
    "#     input_features (default = 11): Number of input features to give the generic model\n",
    "# Outputs: None\n",
    "def test_generic_n_layer(n, hidden_sizes, silent=True, input_features=11):\n",
    "    assert n > 0\n",
    "    assert len(hidden_sizes) == n\n",
    "    \n",
    "    # Use cartesian product to cleverly hide recursive loops\n",
    "    # built with a generator too, so it doesn't eat RAM\n",
    "    accuracies = []\n",
    "    cool_product = itertools.product(*hidden_sizes)\n",
    "    for pattern in cool_product:\n",
    "        layers = build_generic_layers(list(pattern), input_features)\n",
    "        try:\n",
    "            accs = test_tuning(layers)\n",
    "        except KeyboardInterrupt as e:\n",
    "            print(\"test_generic_n_layer: Training ended early due to Keyboard Interrupt. Raising an exception.\")\n",
    "            raise e\n",
    "        if not silent:\n",
    "            print(\"Tested with hidden sizes\", pattern)\n",
    "            print(get_accuracies_string(accs))\n",
    "            print()\n",
    "        accuracies.append(accs)\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1a587e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(52.46153846153846, 93.92307692307692),\n",
       "  (55.10204081632652, 96.12244897959184),\n",
       "  (58.4375, 96.25)],\n",
       " [(53.92307692307692, 93.15384615384616),\n",
       "  (54.285714285714285, 95.61224489795919),\n",
       "  (55.625, 95.0)],\n",
       " [(52.53846153846153, 93.76923076923077),\n",
       "  (54.18367346938775, 95.81632653061224),\n",
       "  (53.43750000000001, 96.25)],\n",
       " [(48.38461538461539, 90.07692307692308),\n",
       "  (55.714285714285715, 95.81632653061224),\n",
       "  (56.25, 95.9375)]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's how you can use all that code!\n",
    "# This line tests a single hidden layer with the following possible dims:\n",
    "#   11 -> 5  -> 11\n",
    "#   11 -> 10 -> 11\n",
    "#   11 -> 15 -> 11\n",
    "test_generic_n_layer(1, ([5, 10, 15],))\n",
    "\n",
    "# This line will test 2 hidden layers with the following combinations:\n",
    "#   11 -> 15 -> 5  -> 11\n",
    "#   11 -> 15 -> 10 -> 11\n",
    "#   11 -> 20 -> 5  -> 11\n",
    "#   11 -> 20 -> 10 -> 11\n",
    "test_generic_n_layer(2, ([15, 20], [5, 10],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d4576cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to surround a string with quotes\n",
    "#\n",
    "# Inputs:\n",
    "#     tuple_string: a string you want to surround with quotes\n",
    "# Outputs:\n",
    "#     result: tuple string with quotes on either side, e.g. \"\\\"test\\\"\"\n",
    "def reformat_tuple_string(tuple_string):\n",
    "    assert isinstance(tuple_string, str)\n",
    "    result = \"\\\"\" + tuple_string + \"\\\"\"\n",
    "    return result\n",
    "\n",
    "#\"(5, 10, 15)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ca26c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a given array to a csv file\n",
    "# Inputs:\n",
    "#     name: string File name (not including extension)\n",
    "#     data: 2_D array_like that is ready to be saved as-is \n",
    "#     delimiter: character or string used to separate data's elements\n",
    "# Outputs:\n",
    "#     Saves a file \"{name}.csv\"\n",
    "def save_as_csv(name, data, delimiter):\n",
    "    \n",
    "    assert isinstance(name, str)\n",
    "    assert isinstance(delimiter, str)\n",
    "    data = np.array(data)\n",
    "    np.savetxt(f\"{name}.csv\", data, delimiter=str(delimiter), fmt=\"%s\")\n",
    "    \n",
    "#save_as_csv(\"test_formatting\", [reformat_tuple_string(\"(5, 10, 15)\")], \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42c94f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs:\n",
    "#     accuracies: list of lists of tuples. \n",
    "#         Each index i contains 2 measure of accuracy for 3 models in the form [(acc1, acc2), ..., (acc1, acc2)]\n",
    "#     hidden_sizes: tuple of lists of hidden sizes \n",
    "#     range_flag (default = True): \n",
    "#         Indicator of whether the data includes multiple models of different numbers of layers or just combinations\n",
    "#         of hidden state sizes for a model of one specific number of layers. \n",
    "# Outputs:\n",
    "#     accuracies_formatted:\n",
    "#         2-D array where each row represents the results for a particular combination of hidden sizes\n",
    "#         There should be 6 columns, the first 3 of which represent exact accuracy and the latter 3 of which represent\n",
    "#         accuracy with a tolerance of 1 rating point (i.e. adjacent classification in the quality scale).  \n",
    "def format_accuracies(accuracies, hidden_sizes, range_flag=True):\n",
    "    print(f\"format_accuracies: len(accuracies) = {len(accuracies)}, len(hidden_sizes) = {len(hidden_sizes)}\")\n",
    "    print(f\"format_accuracies: accuracies = {accuracies}\")\n",
    "    accuracies_formatted = []        \n",
    "    accuracies_temp = []\n",
    "    \n",
    "    #Determines if the loop runs once or once per hidden_sizes combination per model in [1, n]\n",
    "    range_flag_switch = len(hidden_sizes) \n",
    "    if not range_flag:\n",
    "        range_flag_switch = 1\n",
    "    print(f\"format_accuracies: range_flag_switch = {range_flag_switch}\")\n",
    "    \n",
    "   \n",
    "    for n in range(range_flag_switch):\n",
    "        hidden_sizes_cur = hidden_sizes[:n+1] if range_flag else hidden_sizes\n",
    "        cartesian_product = itertools.product(*hidden_sizes_cur)\n",
    "        idx = 0\n",
    "        for combination in cartesian_product:\n",
    "            accuracy_cur = accuracies[n][idx]\n",
    "            temp_row = []\n",
    "            temp_row.append(reformat_tuple_string(str(combination)))\n",
    "            for i in range(3):\n",
    "                for j in range(2):\n",
    "                    temp_row.append(str(accuracy_cur[i][j]))\n",
    "            accuracies_temp.append(temp_row)\n",
    "            idx += 1\n",
    "    accuracies_formatted = accuracies_temp\n",
    "    \n",
    "    print(f\"format_accuracies: accuracies_formatted(len = {len(accuracies_formatted)}) = {accuracies_formatted}\")\n",
    "    total_accuracies = 0\n",
    "    for i in range(len(accuracies)):\n",
    "        total_accuracies += len(accuracies[i])\n",
    "    assert len(accuracies_formatted) == total_accuracies\n",
    "    \n",
    "    return accuracies_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8fe6f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the hidden_sizes tuple used by the generic network\n",
    "#     testing methods\n",
    "# Inputs:\n",
    "#     n: Number of layers to generate hidden_sizes for\n",
    "#     starts_and_ends: length n list of integer tuples of the form [(start, end), ..., (start, end)]\n",
    "#     step_sizes: length n list of integer step sizes for each \n",
    "#         number of layers, [1, n]\n",
    "# Outputs:\n",
    "#     hidden_sizes: tuple of lists of hidden sizes\n",
    "def generate_hidden_sizes(n, starts_and_ends, step_sizes):\n",
    "    assert n > 0\n",
    "    assert len(starts_and_ends) == n\n",
    "    assert len(step_sizes) == n\n",
    "    assert isinstance(starts_and_ends, list)\n",
    "    assert isinstance(step_sizes, list)\n",
    "    hidden_sizes = None\n",
    "    for i in range(n):\n",
    "        start, end = starts_and_ends[i]\n",
    "        step_size = step_sizes[i]\n",
    "        hidden_sizes_n = list(range(start, end+1, step_size))\n",
    "        if hidden_sizes is None:\n",
    "            hidden_sizes = (hidden_sizes_n, )\n",
    "        else:\n",
    "            hidden_sizes += (hidden_sizes_n, )\n",
    "    return hidden_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1083ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test_generic_n_layer for layer sizes [1, max_n], aggregate the results and save them to a .csv file\n",
    "# Inputs:\n",
    "#     max_n: maximum number of layers to test through starting at 1\n",
    "#     hidden_sizes: max_n length tuple of hidden size lists\n",
    "#     filename: string excluding file extension to name the saved data file\n",
    "#     delimiter: delimiter between elements in the csv file (since it is acceptable to Excel to use things that aren't commas)\n",
    "#     silent (default = True): Determines whether debug printouts will run\n",
    "#     input_features (default = 11): Number of input features to give the models being tested\n",
    "# Outputs:\n",
    "#     Saves the resulting data to a file\n",
    "def test_and_save(max_n, hidden_sizes, filename, delimiter, silent=True, input_features=11):\n",
    "    accuracies_max_n = []\n",
    "    assert len(hidden_sizes) == max_n\n",
    "    for n in range(max_n):\n",
    "        hidden_sizes_cur = hidden_sizes[:n+1]\n",
    "        assert len(hidden_sizes_cur) == n+1\n",
    "        try:\n",
    "            accuracies = test_generic_n_layer(n+1, hidden_sizes_cur, silent, input_features)\n",
    "        except KeyboardInterrupt as e:\n",
    "            print(\"test_and_save: Training ended early due to Keyboard Interrupt. Raising an exception.\")\n",
    "            raise e\n",
    "        if not silent: print(accuracies)\n",
    "        accuracies_max_n.append(accuracies)\n",
    "    \n",
    "    print(f\"accuracies generated (but not yet formatted): {accuracies_max_n}\")#remove this\n",
    "    data = format_accuracies(accuracies_max_n, hidden_sizes)\n",
    "    save_as_csv(filename, data, delimiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4823d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_hidden_sizes simple test result = ([5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100],)\n",
      "Exiting from training early\n",
      "train_and_test: Keyboard Interrupt detected, raising an exception\n",
      "create_and_report_model. Training ended early. Raising an exception\n",
      "test_tuning: Training ended early due to keyboard interrupt. Raising an exception\n",
      "test_generic_n_layer: Training ended early due to Keyboard Interrupt. Raising an exception.\n",
      "test_and_save: Training ended early due to Keyboard Interrupt. Raising an exception.\n",
      "Received a KeyboardInterrupt. Try testing again when you're ready.\n"
     ]
    }
   ],
   "source": [
    "print(f\"generate_hidden_sizes simple test result = {generate_hidden_sizes(1, [(5, 100)], [5])}\")   \n",
    "\n",
    "\n",
    "try:\n",
    "    #test_and_save(3, generate_hidden_sizes(3, [(5, 100), (5, 100), (5, 100)], [5, 5, 5]), \"Big_Test_n_equals_3_h_spans_5_to_100\")\n",
    "    #test_and_save(1, generate_hidden_sizes(1, [(5, 10)], [5]), \"small_test_less_noisy\", \";\", False)\n",
    "    #test_and_save(1, generate_hidden_sizes(1, [(5, 15)], [5]), \"test_nequals1_hsizerange5to100\", \";\", False)\n",
    "    #test_and_save(3, generate_hidden_sizes(3, [(5, 15), (5, 15), (5, 15)], [5, 5, 5]), \"test2_nequals3_hsizerange5to15\", \";\", False)\n",
    "except KeyboardInterrupt as e:\n",
    "    print(\"Received a KeyboardInterrupt. Try testing again when you're ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775c0f6",
   "metadata": {},
   "source": [
    "## Computational Experiment #1:\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "For our first computational experiment we want to try and isolate the data's most important feature. To accomplish that, we're going to try two different methods:\n",
    "- Retraining models that take 10 input features on every length-10 subset of the 11 data features\n",
    "- Using our optimal trained model from hypertuning and zeroing out one feature per trial.\n",
    "\n",
    "Hypothesis: There is a most important feature in the data for classifying the quality of a wine, and all the features are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26dd5309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature selection on a new model for each 10-length subset of the input features\n",
    "# Inputs:\n",
    "#     hidden_sizes: List of hidden sizes in order to build a model with.\n",
    "#     silent: Determines whether debug printouts will run.\n",
    "#     input_features: Number of input features \n",
    "# Outputs:\n",
    "#     Prints a list of accuracies for all 11 feature selection experiments\n",
    "def feature_select_retrain(hidden_sizes, silent=True, input_features=10):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    accuracy = []\n",
    "    # We have: data_array, data_array_white, data_array_red as well\n",
    "    for i in range(11):\n",
    "        # Remove one column of the data arrays to test how the model does without it\n",
    "        cur_data_array = np.delete(np.copy(data_array), i, 1)\n",
    "        cur_data_array_white = np.delete(np.copy(data_array_white), i, 1)\n",
    "        cur_data_array_red = np.delete(np.copy(data_array_red), i, 1)\n",
    "        \n",
    "        loaders = get_random_loaders(cur_data_array, cur_data_array_white, cur_data_array_red)\n",
    "        true_lists = []\n",
    "        pred_lists = []\n",
    "        for j in range(3):\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            cur_loaders = loaders[j]\n",
    "            cur_layers = build_generic_layers(hidden_sizes, input_features)\n",
    "            cur_model = WineQualityModel(cur_layers)\n",
    "            cur_optimizer = torch.optim.Adam(cur_model.parameters(), lr=1e-3)\n",
    "            try:\n",
    "                acc, true, pred = train_and_test(cur_model, *cur_loaders, criterion, \\\n",
    "                                         cur_optimizer, nepoch=250, mode=f\"Model ({i}, {j})\", silent=silent)\n",
    "            except KeyboardInterrupt as e:\n",
    "                print(\"feature_select_retrain Training ended early. Exiting\")\n",
    "                return\n",
    "            true_lists.append(true)\n",
    "            pred_lists.append(pred)\n",
    "            \n",
    "        accs = []\n",
    "        for j in range(3):\n",
    "            cur_exact_acc, cur_t1 = compute_model_accuracies(true_lists[j], pred_lists[j])\n",
    "            tuple_acc = (cur_exact_acc, cur_t1)\n",
    "            accs.append(tuple_acc)\n",
    "        acc_str = get_accuracies_string(accs)\n",
    "        accuracy.append(accs)\n",
    "        print(f\"feature_select_retrain Trial {i}: Feature at index {i} not being considered. Accuracies = {acc_str}\")\n",
    "    \n",
    "    # Loop through each trial\n",
    "    worst_exacts = [-1, -1, -1]\n",
    "    worst_t1s = [-1, -1, -1]\n",
    "    best_exacts = [-1, -1, -1]\n",
    "    best_t1s = [-1, -1, -1]\n",
    "    \n",
    "    idx = 0\n",
    "    for trial in accuracy:\n",
    "        for i in range(3):\n",
    "            cur_tuple = trial[i]\n",
    "            if worst_exacts[i] == -1 or cur_tuple[0] < accuracy[worst_exacts[i]][i][0]:\n",
    "                worst_exacts[i] = idx\n",
    "            if worst_t1s[i] == -1 or cur_tuple[0] < accuracy[worst_t1s[i]][i][0]:\n",
    "                worst_t1s[i] = idx\n",
    "            if best_exacts[i] == -1 or cur_tuple[0] > accuracy[best_exacts[i]][i][0]:\n",
    "                best_exacts[i] = idx\n",
    "            if best_t1s[i] == -1 or cur_tuple[0] > accuracy[best_t1s[i]][i][0]:\n",
    "                best_t1s[i] = idx\n",
    "        idx += 1\n",
    "    \n",
    "    print(f\"Worst Trial for Combined Data Set Exact Accuracy = {worst_exacts[0]}\")\n",
    "    print(f\"Worst Trial for White Wine Data Set Exact Accuracy = {worst_exacts[1]}\")\n",
    "    print(f\"Worst Trial for Red Wine Data Set Exact Accuracy = {worst_exacts[2]}\")\n",
    "    print(f\"Worst Trial for Combined Data Set T1 Accuracy = {worst_t1s[0]}\")\n",
    "    print(f\"Worst Trial for White Wine Data Set T1 Accuracy = {worst_t1s[1]}\")\n",
    "    print(f\"Worst Trial for Red Wine Data Set T1 Accuracy = {worst_t1s[2]}\")\n",
    "    print(f\"Best Trial for Combined Data Set Exact Accuracy = {best_exacts[0]}\")\n",
    "    print(f\"Best Trial for White Wine Data Set Exact Accuracy = {best_exacts[1]}\")\n",
    "    print(f\"Best Trial for Red Wine Data Set Exact Accuracy = {best_exacts[2]}\")\n",
    "    print(f\"Best Trial for Combined Data Set T1 Accuracy = {best_t1s[0]}\")\n",
    "    print(f\"Best Trial for White Wine Data Set T1 Accuracy = {best_t1s[1]}\")\n",
    "    print(f\"Best Trial for Red Wine Data Set T1 Accuracy = {best_t1s[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6056d38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0: Feature at index 0 not being considered. Accuracies = [(53.15,93.23),(53.16,95.20),(57.19,95.94)\n",
      "Trial 1: Feature at index 1 not being considered. Accuracies = [(52.08,94.77),(51.63,94.90),(59.06,96.25)\n",
      "Trial 2: Feature at index 2 not being considered. Accuracies = [(52.15,93.85),(51.94,92.55),(56.88,94.69)\n",
      "Trial 3: Feature at index 3 not being considered. Accuracies = [(52.92,94.31),(51.02,93.67),(54.37,95.62)\n",
      "Trial 4: Feature at index 4 not being considered. Accuracies = [(52.23,94.77),(48.98,93.78),(52.81,95.00)\n",
      "Trial 5: Feature at index 5 not being considered. Accuracies = [(52.85,95.38),(51.73,94.29),(60.00,98.44)\n",
      "Trial 6: Feature at index 6 not being considered. Accuracies = [(53.77,96.46),(48.88,93.78),(62.81,97.19)\n",
      "Trial 7: Feature at index 7 not being considered. Accuracies = [(54.00,94.15),(50.00,92.96),(62.19,96.56)\n",
      "Trial 8: Feature at index 8 not being considered. Accuracies = [(55.08,93.77),(53.88,92.35),(57.81,95.00)\n",
      "Trial 9: Feature at index 9 not being considered. Accuracies = [(54.23,94.92),(52.14,93.98),(54.69,95.62)\n",
      "Trial 10: Feature at index 10 not being considered. Accuracies = [(48.62,94.77),(47.65,94.80),(57.81,95.00)\n",
      "Worst Trial for Combined Data Set Exact Accuracy = 10\n",
      "Worst Trial for White Wine Data Set Exact Accuracy = 10\n",
      "Worst Trial for Red Wine Data Set Exact Accuracy = 4\n",
      "Worst Trial for Combined Data Set T1 Accuracy = 10\n",
      "Worst Trial for White Wine Data Set T1 Accuracy = 10\n",
      "Worst Trial for Red Wine Data Set T1 Accuracy = 4\n",
      "Best Trial for Combined Data Set Exact Accuracy = 8\n",
      "Best Trial for White Wine Data Set Exact Accuracy = 8\n",
      "Best Trial for Red Wine Data Set Exact Accuracy = 6\n",
      "Best Trial for Combined Data Set T1 Accuracy = 8\n",
      "Best Trial for White Wine Data Set T1 Accuracy = 8\n",
      "Best Trial for Red Wine Data Set T1 Accuracy = 6\n"
     ]
    }
   ],
   "source": [
    "feature_select_retrain([15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d74ef87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_dropout(model):\n",
    "    accuracy = []\n",
    "    for i in range(11):\n",
    "        cur_data_array = np.copy(data_array)\n",
    "        cur_data_array_white = np.copy(data_array_white)\n",
    "        cur_data_array_red = np.copy(data_array_red)\n",
    "        cur_data_array[:, i] = 0\n",
    "        cur_data_array_white[:, i] = 0\n",
    "        cur_data_array_red[:, i] = 0\n",
    "        \n",
    "        loaders = get_random_loaders(cur_data_array, cur_data_array_white, cur_data_array_red)\n",
    "        true_lists = []\n",
    "        pred_lists = []\n",
    "        for j in range(3):\n",
    "            cur_optimizer = torch.optim.Adam(cur_model.parameters(), lr=1e-3)\n",
    "            cur_loaders = loaders[j]\n",
    "            acc, true, pred = test_network(model, cur_loaders[2])\n",
    "        \n",
    "            true_lists.append(true)\n",
    "            pred_lists.append(pred)\n",
    "            \n",
    "        accs = []\n",
    "        for j in range(3):\n",
    "            cur_exact_acc, cur_t1 = compute_model_accuracies(true_lists[j], pred_lists[j])\n",
    "            tuple_acc = (cur_exact_acc, cur_t1)\n",
    "            accs.append(tuple_acc)\n",
    "        acc_str = get_accuracies_string(accs)\n",
    "        accuracy.append(accs)\n",
    "        print(f\"feature_select_dropout Trial {i}: Feature at index {i} not being considered. Accuracies = {acc_str}\")\n",
    "    \n",
    "    # Loop through each trial\n",
    "    worst_exacts = [-1, -1, -1]\n",
    "    worst_t1s = [-1, -1, -1]\n",
    "    best_exacts = [-1, -1, -1]\n",
    "    best_t1s = [-1, -1, -1]\n",
    "    \n",
    "    idx = 0\n",
    "    for trial in accuracy:\n",
    "        for i in range(3):\n",
    "            cur_tuple = trial[i]\n",
    "            if worst_exacts[i] == -1 or cur_tuple[0] < accuracy[worst_exacts[i]][i][0]:\n",
    "                worst_exacts[i] = idx\n",
    "            if worst_t1s[i] == -1 or cur_tuple[0] < accuracy[worst_t1s[i]][i][0]:\n",
    "                worst_t1s[i] = idx\n",
    "            if best_exacts[i] == -1 or cur_tuple[0] > accuracy[best_exacts[i]][i][0]:\n",
    "                best_exacts[i] = idx\n",
    "            if best_t1s[i] == -1 or cur_tuple[0] > accuracy[best_t1s[i]][i][0]:\n",
    "                best_t1s[i] = idx\n",
    "        idx += 1\n",
    "    \n",
    "    print(f\"Worst Trial for Combined Data Set Exact Accuracy = {worst_exacts[0]}\")\n",
    "    print(f\"Worst Trial for White Wine Data Set Exact Accuracy = {worst_exacts[1]}\")\n",
    "    print(f\"Worst Trial for Red Wine Data Set Exact Accuracy = {worst_exacts[2]}\")\n",
    "    print(f\"Worst Trial for Combined Data Set T1 Accuracy = {worst_t1s[0]}\")\n",
    "    print(f\"Worst Trial for White Wine Data Set T1 Accuracy = {worst_t1s[1]}\")\n",
    "    print(f\"Worst Trial for Red Wine Data Set T1 Accuracy = {worst_t1s[2]}\")\n",
    "    print(f\"Best Trial for Combined Data Set Exact Accuracy = {best_exacts[0]}\")\n",
    "    print(f\"Best Trial for White Wine Data Set Exact Accuracy = {best_exacts[1]}\")\n",
    "    print(f\"Best Trial for Red Wine Data Set Exact Accuracy = {best_exacts[2]}\")\n",
    "    print(f\"Best Trial for Combined Data Set T1 Accuracy = {best_t1s[0]}\")\n",
    "    print(f\"Best Trial for White Wine Data Set T1 Accuracy = {best_t1s[1]}\")\n",
    "    print(f\"Best Trial for Red Wine Data Set T1 Accuracy = {best_t1s[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20acc309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.     0.27   0.36  ...  0.45   8.8    6.   ]\n",
      " [ 0.     0.3    0.34  ...  0.49   9.5    6.   ]\n",
      " [ 0.     0.28   0.4   ...  0.44  10.1    6.   ]\n",
      " ...\n",
      " [ 0.     0.51   0.13  ...  0.75  11.     6.   ]\n",
      " [ 0.     0.645  0.12  ...  0.71  10.2    5.   ]\n",
      " [ 0.     0.31   0.47  ...  0.66  11.     6.   ]]\n"
     ]
    }
   ],
   "source": [
    "model = WineQualityModel((\n",
    "    torch.nn.Linear(11, 81),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(81, 11)\n",
    "))\n",
    "\n",
    "feature_select_dropout(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98b39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
