{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849a57cd",
   "metadata": {},
   "source": [
    "# CS 342 Neural Nets: Final Project\n",
    "\n",
    "Authors: Ryan Gahagan (rg32643) and Dustan Helm (dbh878)\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this project, we will create a neural network that will hopefully be able to predict the quality of various wines given their chemical compositions.\n",
    "\n",
    "This project's data set and idea are based off another paper, cited here:\n",
    "\n",
    "  \"P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "  \n",
    "  Modeling wine preferences by data mining from physicochemical properties.\n",
    "  \n",
    "  In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\"\n",
    "\n",
    "We plan to make a feed-forward network to process this data, as well as some experiments to test and analyze our network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51836a78",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "First, we have to take the data set and process it into a format that Python can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60be6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block to load important libraries and set things up\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb994c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in dataset files\n",
    "white_file = open(\"winequality-white-nolabels.csv\")\n",
    "wine_quality_white = np.loadtxt(white_file, delimiter=\";\")\n",
    "red_file = open(\"winequality-red-nolabels.csv\")\n",
    "wine_quality_red = np.loadtxt(red_file, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b4d724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine_quality_combined_whitered.shape = (6497, 12)\n"
     ]
    }
   ],
   "source": [
    "# Create suitable data_arrays\n",
    "num_samples_white = wine_quality_white.shape[0]\n",
    "num_samples_red = wine_quality_red.shape[0]\n",
    "num_samples_total = num_samples_white + num_samples_red\n",
    "\n",
    "# Combine white and red wine datasets into one np array\n",
    "wine_quality_combined_whitered = np.append(wine_quality_white, wine_quality_red, axis=0)\n",
    "\n",
    "print(f\"wine_quality_combined_whitered.shape = {wine_quality_combined_whitered.shape}\")\n",
    "\n",
    "assert wine_quality_combined_whitered.shape[0] == num_samples_total\n",
    "\n",
    "# Rename\n",
    "data_array = wine_quality_combined_whitered\n",
    "data_array_white = wine_quality_white\n",
    "data_array_red = wine_quality_red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c0585",
   "metadata": {},
   "source": [
    "Now we've loaded our datasets into lists. `data_array_red` is an `np.array` whose shape is `(1599,12)` and `data_array_white` is likewise an array of shape `(4898,12)`. `data_array` is simply those two arrays concatenated into an array of shape `(6497,12)`.\n",
    "\n",
    "Note that these 12 columns represent both features and labels.\n",
    "\n",
    "The columns, in order, are:\n",
    "- fixed acidity\n",
    "- volatile acidity\n",
    "- citric acid\n",
    "- residual sugar\n",
    "- chlorides\n",
    "- free sulfur dioxide\n",
    "- total sulfur dioxide\n",
    "- density\n",
    "- pH\n",
    "- sulphates\n",
    "- alcohol\n",
    "- quality (integer in \\[0,10\\])\n",
    "where quality is our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd608f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data array into features and label arrays\n",
    "# Inputs: \n",
    "#     data_array: Data array to be split into features and labels\n",
    "# Outputs:\n",
    "#     data_array_feats: Features for data_array\n",
    "#     data_array_labels: Labels for data_array\n",
    "def split_features_labels(data_array):\n",
    "    assert isinstance(data_array, np.ndarray)\n",
    "    assert data_array.shape[1] == 12\n",
    "    \n",
    "    data_array_feats = data_array[:,:-1] # first 11 columns\n",
    "    data_array_labels = data_array[:,-1] # last column\n",
    "\n",
    "    assert data_array_feats.shape[1] == 11\n",
    "    assert data_array_feats.shape[0] == data_array_labels.shape[0]\n",
    "    \n",
    "    return data_array_feats, data_array_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e892a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data array into training and testing sets based on the provided train_proportion parameter\n",
    "# Inputs: \n",
    "#     data_array: Dataset to split for training and testing\n",
    "#     train_proportion: Proportion of datapoints to be kept as training data\n",
    "# Outputs:\n",
    "#     train_set: Training set containing a proportion of the datapoints contained in data_array specified by input parameter.\n",
    "#     test_set: Testing set containing held-back datapoints to test the trained model\n",
    "def train_test_split(data_array, train_proportion):\n",
    "    assert isinstance(data_array, np.ndarray)\n",
    "    assert data_array.shape[1] == 12\n",
    "    \n",
    "    num_samples = data_array.shape[0]\n",
    "    \n",
    "    feats, labels = split_features_labels(data_array)\n",
    "    data_set = torch.utils.data.TensorDataset(torch.tensor(feats), torch.tensor(labels).long())\n",
    "    \n",
    "    train_size = int(train_proportion*num_samples)\n",
    "    test_size = num_samples - train_size\n",
    "    \n",
    "    train_set, test_set = torch.utils.data.random_split(data_set, [train_size, test_size])\n",
    "    \n",
    "    assert abs(len(train_set) / len(data_array) - train_proportion) < 0.01\n",
    "    assert abs(len(test_set) / len(data_array)  - (1 - train_proportion)) < 0.01\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a870039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the actual split on all of our datasets into training and testing\n",
    "train_proportion = 0.8\n",
    "train_proportion_white = 0.8\n",
    "train_proportion_red = 0.8\n",
    "\n",
    "train_set, test_set = train_test_split(data_array, train_proportion)\n",
    "train_set_white, test_set_white = train_test_split(data_array_white, train_proportion_white)\n",
    "train_set_red, test_set_red = train_test_split(data_array_red, train_proportion_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b35420ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set into true training and validation data based on the input proportion\n",
    "# Inputs:\n",
    "#     ntotal: Total number of datapoints in the original training set to be used to determine the split\n",
    "#     train_proportion: Proportion of the training set examples which should not be placed into the validation set\n",
    "# Outputs:\n",
    "#     train_ix: Indices for training examples\n",
    "#     val_ix: Indices for validation examples\n",
    "\n",
    "def train_val_split_ix(ntotal, train_proportion):\n",
    "    ntrain = int(train_proportion*ntotal)\n",
    "    nval = ntotal - ntrain\n",
    "    \n",
    "    val_ix = np.random.choice(range(ntotal), size=nval, replace=False)\n",
    "    train_ix = list(set(range(ntotal)) - set(val_ix))\n",
    "    \n",
    "    assert abs(len(train_ix) / ntotal - train_proportion) < 0.01\n",
    "    assert abs(len(val_ix) / ntotal - (1 - train_proportion)) < 0.01\n",
    "    \n",
    "    return (train_ix, val_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb14a88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(len(train_ix), len(val_ix)) = (4677, 520)\n",
      "(len(train_white_ix), len(val_white_ix)) = (3526, 392)\n",
      "(len(train_red_ix), len(val_red_ix)) = (1151, 128)\n"
     ]
    }
   ],
   "source": [
    "# Perform the training/validation split and then confirm array lengths\n",
    "train_proportion2 = 0.9\n",
    "train_proportion_white2 = 0.9\n",
    "train_proportion_red2 = 0.9\n",
    "\n",
    "train_ix, val_ix = train_val_split_ix(len(train_set), train_proportion2)\n",
    "train_white_ix, val_white_ix = train_val_split_ix(len(train_set_white), train_proportion_white2)\n",
    "train_red_ix, val_red_ix = train_val_split_ix(len(train_set_red), train_proportion_red2)\n",
    "\n",
    "print(f\"(len(train_ix), len(val_ix)) = ({len(train_ix)}, {len(val_ix)})\")\n",
    "print(f\"(len(train_white_ix), len(val_white_ix)) = ({len(train_white_ix)}, {len(val_white_ix)})\")\n",
    "print(f\"(len(train_red_ix), len(val_red_ix)) = ({len(train_red_ix)}, {len(val_red_ix)})\")\n",
    "\n",
    "assert len(train_ix) + len(val_ix) == len(train_set)\n",
    "assert len(train_white_ix) + len(val_white_ix) == len(train_set_white)\n",
    "assert len(train_red_ix) + len(val_red_ix) == len(train_set_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e44ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data samplers for use in DataLoader objects\n",
    "# Inputs: \n",
    "#     datalist_ix: Tuple of index lists used to determine each loader's data\n",
    "# Outputs:\n",
    "#     result: Tuple of SubsetRandomSamplers representing each index list object in datalist_ix\n",
    "def setup_samplers(datalist_ix):\n",
    "    result = ()\n",
    "    for data_ix in datalist_ix:\n",
    "        result += (SubsetRandomSampler(data_ix),)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf04a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a tuple of Data Loaders based on provided datasets, samplers, and batch_size\n",
    "# Inputs:\n",
    "#     datalist: List of datasets to give to the DataLoaders\n",
    "#     samplers: List of samplers to use in the DataLoaders\n",
    "#     batch_size: DataLoader batch size (Currently uses the same batch_size for every dataset passed)\n",
    "# Outputs:\n",
    "#     Tuple of DataLoader objects len(datalist) size long \n",
    "def setup_data_loaders(datalist, samplers, batch_size):\n",
    "    assert len(datalist) == len(samplers)\n",
    "    \n",
    "    result = ()\n",
    "    for i in range(len(datalist)):\n",
    "        data = datalist[i]\n",
    "        sampler = samplers[i]\n",
    "        result += (torch.utils.data.DataLoader(data, batch_size, sampler=sampler),)\n",
    "    return result\n",
    "\n",
    "#TODO: It may be useful to have different batch_size values for training, validation, and testing. \n",
    "# In that event, batch_size should be replaced with a touple of batch_size values and the following should be added to the loop:\n",
    "# batch_size = batch_sizes[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f05333",
   "metadata": {},
   "source": [
    "Now that we've declared methods to do various helper tasks, we're going to actually break our data into useful information.\n",
    "\n",
    "The three sets of data (red, white, and combined) will be each partitioned into a training set, a validation set, and a testing set (whose sizes will be proportional to the variables declared above). We will then create `DataLoader` objects for each of these partitions so that we can iterate over them in our training section.\n",
    "\n",
    "Note here that we also declare batch sizes to determine how many pieces of information are trained on at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01ca66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up samplers and DataLoaders for all three datasets (Combined, White, Red)\n",
    "batch_size = 100\n",
    "batch_size_white = 100\n",
    "batch_size_red = 100\n",
    "\n",
    "#COMBINED DATASET\n",
    "sampler_input = (train_ix, val_ix)\n",
    "train_sampler, val_sampler = setup_samplers(sampler_input)\n",
    "\n",
    "datalist = (train_set, train_set, test_set)\n",
    "samplers = (train_sampler, val_sampler, None)\n",
    "train_loader, val_loader, test_loader = setup_data_loaders(datalist, samplers, batch_size)\n",
    "\n",
    "#JUST WHITE\n",
    "sampler_input_white = (train_white_ix, val_white_ix)\n",
    "train_sampler_white, val_sampler_white = setup_samplers(sampler_input_white)\n",
    "\n",
    "datalist_white = (train_set_white, train_set_white, test_set_white)\n",
    "samplers_white = (train_sampler_white, val_sampler_white, None)\n",
    "train_loader_white, val_loader_white, test_loader_white = setup_data_loaders(datalist_white, samplers_white, batch_size_white)\n",
    "\n",
    "#JUST RED\n",
    "sampler_input_red = (train_red_ix, val_red_ix)\n",
    "train_sampler_red, val_sampler_red = setup_samplers(sampler_input_red)\n",
    "\n",
    "datalist_red = (train_set_red, train_set_red, test_set_red)\n",
    "samplers_red = (train_sampler_red, val_sampler_red, None)\n",
    "train_loader_red, val_loader_red, test_loader_red = setup_data_loaders(datalist_red, samplers_red, batch_size_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96077d94",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "With our data processed and ready to be used, now we will write some functions to train and test a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fd3f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the provided model with data gathered from train_loader, the given criterion, and the given optimizer.\n",
    "# Additionally perform validation checks with data from val_loader.\n",
    "# Inputs:\n",
    "#     model: Neural network to train\n",
    "#     train_loader: DataLoader which provides training data to the model \n",
    "#     val_loader: DataLoader which provides validation data to the model\n",
    "#     criterion: Loss Function which trains the model\n",
    "#     optimizer: Optimization algorithm to improve loss during training \n",
    "#     nepoch: Number of epochs to train for (Defaults to 100)\n",
    "# Outputs:\n",
    "#     Prints the Training and Validation loss at each epoch\n",
    "def train_network(model, train_loader, val_loader, criterion, optimizer, nepoch=100,silent=True):\n",
    "    try:\n",
    "        cur_range = tqdm(range(nepoch))\n",
    "        if silent:\n",
    "            cur_range = range(nepoch)\n",
    "        for epoch in cur_range:\n",
    "            # Train over each epoch with a progress bar (tqdm)\n",
    "            if not silent: \n",
    "                print('EPOCH %d'%epoch)\n",
    "            \n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for inputs, labels in train_loader:\n",
    "                # For each train input: \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward propagate inputs\n",
    "                outputs = model.forward(inputs)\n",
    "                \n",
    "                # Compute loss and learn\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Add current loss to batch average\n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "            \n",
    "            # Show Training loss for current epoch over the train_loader data\n",
    "            if not silent:\n",
    "                print('{:>12s} {:>7.5f}'.format('Train loss:', total_loss/count))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Perform Validation checks on the newly trained model\n",
    "                total_loss = 0\n",
    "                count = 0\n",
    "                for inputs, labels in val_loader:\n",
    "                    # Forward propagate inputs\n",
    "                    outputs = model.forward(inputs)\n",
    "                    \n",
    "                    # Compute Loss\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Add current loss to batch average\n",
    "                    total_loss += loss.item()\n",
    "                    count += 1\n",
    "                    \n",
    "                # Show Validation loss for current epoch over the val_loader data\n",
    "                if not silent:\n",
    "                    print('{:>12s} {:>7.5f}'.format('Val loss:', total_loss/count))\n",
    "            print()\n",
    "    except KeyboardInterrupt:\n",
    "        print('Exiting from training early')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c21a1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the provided model with data from test_loader\n",
    "# Inputs:\n",
    "#     model: Model to test using unseen data\n",
    "#     test_loader: DataLoader to provide held-back testing data to trained model\n",
    "#     mode: String used at front of each loss printout\n",
    "# Outputs:\n",
    "#     acc: Top-1 accuracy of the model on the testing data in percent\n",
    "#     true: Array of actual labels\n",
    "#     pred: Array of model-predicted labels\n",
    "def test_network(model, test_loader, mode, silent=True):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true, pred = [], []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, labels  in test_loader:\n",
    "            # Forward propagate testing data\n",
    "            outputs = model.forward(inputs)\n",
    "            \n",
    "            # Get the prediction for inputs\n",
    "            vals, predicted = torch.max(outputs, dim=1) \n",
    "            \n",
    "            # Tally results \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            true.append(labels)\n",
    "            pred.append(predicted)\n",
    "    \n",
    "    # Compute and print final accuracy, then format outputs\n",
    "    acc = (100 * correct / total)\n",
    "    if not silent:\n",
    "        print('%s accuracy: %0.3f' % (mode, acc))\n",
    "    true = np.concatenate(true)\n",
    "    pred = np.concatenate(pred)\n",
    "    return acc, true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9b5ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Kind of) placeholder method that wraps training and testing\n",
    "def train_and_test(model, train_loader, val_loader, test_loader, criterion, optimizer, \\\n",
    "                   nepoch=100, mode=\"Model\", train_silent=True, test_silent=True):\n",
    "    train_network(model, train_loader, val_loader, criterion, optimizer, train_silent)\n",
    "    model.eval()\n",
    "    acc, true, pred = test_network(model, test_loader, mode, test_silent)\n",
    "    return acc, true, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cc903",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "With our training and testing functionality equipped, now we will actually decide how to build our model.\n",
    "\n",
    "Note here that there will be a heavy focus on making the model flexible so that we can tune hyperparameters or test new input varieties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "325ac4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineQualityModel(torch.nn.Module):\n",
    "    # Constructor for a WineQualityModel\n",
    "    # Inputs:\n",
    "    #     layers: a tuple of layers that you want in the model\n",
    "    #       note that the output shape must be of length 10\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # NOTE: this gives the construction tons of flexibility\n",
    "        # but also leaves plenty of room for dimensionality errors\n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers.forward(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f3df1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109c9e09705148f3979138402d7d5e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      " Train loss: 6.78688\n",
      "   Val loss: 5.16468\n",
      "\n",
      "EPOCH 1\n",
      " Train loss: 3.62577\n",
      "   Val loss: 2.55897\n",
      "\n",
      "EPOCH 2\n",
      " Train loss: 2.14245\n",
      "   Val loss: 1.73878\n",
      "\n",
      "EPOCH 3\n",
      " Train loss: 1.68527\n",
      "   Val loss: 1.58852\n",
      "\n",
      "EPOCH 4\n",
      " Train loss: 1.50094\n",
      "   Val loss: 1.47331\n",
      "\n",
      "EPOCH 5\n",
      " Train loss: 1.41662\n",
      "   Val loss: 1.44346\n",
      "\n",
      "EPOCH 6\n",
      " Train loss: 1.35345\n",
      "   Val loss: 1.37156\n",
      "\n",
      "EPOCH 7\n",
      " Train loss: 1.33272\n",
      "   Val loss: 1.40017\n",
      "\n",
      "EPOCH 8\n",
      " Train loss: 1.31496\n",
      "   Val loss: 1.41594\n",
      "\n",
      "EPOCH 9\n",
      " Train loss: 1.29974\n",
      "   Val loss: 1.27166\n",
      "\n",
      "EPOCH 10\n",
      " Train loss: 1.27952\n",
      "   Val loss: 1.32786\n",
      "\n",
      "EPOCH 11\n",
      " Train loss: 1.29297\n",
      "   Val loss: 1.27876\n",
      "\n",
      "EPOCH 12\n",
      " Train loss: 1.28166\n",
      "   Val loss: 1.35561\n",
      "\n",
      "EPOCH 13\n",
      " Train loss: 1.28191\n",
      "   Val loss: 1.37350\n",
      "\n",
      "EPOCH 14\n",
      " Train loss: 1.27386\n",
      "   Val loss: 1.33346\n",
      "\n",
      "EPOCH 15\n",
      " Train loss: 1.27898\n",
      "   Val loss: 1.34314\n",
      "\n",
      "EPOCH 16\n",
      " Train loss: 1.26867\n",
      "   Val loss: 1.26711\n",
      "\n",
      "EPOCH 17\n",
      " Train loss: 1.26171\n",
      "   Val loss: 1.26615\n",
      "\n",
      "EPOCH 18\n",
      " Train loss: 1.26755\n",
      "   Val loss: 1.25451\n",
      "\n",
      "EPOCH 19\n",
      " Train loss: 1.25846\n",
      "   Val loss: 1.27844\n",
      "\n",
      "EPOCH 20\n",
      " Train loss: 1.26185\n",
      "   Val loss: 1.24691\n",
      "\n",
      "EPOCH 21\n",
      " Train loss: 1.25318\n",
      "   Val loss: 1.23296\n",
      "\n",
      "EPOCH 22\n",
      " Train loss: 1.25383\n",
      "   Val loss: 1.27480\n",
      "\n",
      "EPOCH 23\n",
      " Train loss: 1.25337\n",
      "   Val loss: 1.29480\n",
      "\n",
      "EPOCH 24\n",
      " Train loss: 1.24371\n",
      "   Val loss: 1.27142\n",
      "\n",
      "EPOCH 25\n",
      " Train loss: 1.24671\n",
      "   Val loss: 1.27144\n",
      "\n",
      "EPOCH 26\n",
      " Train loss: 1.24894\n",
      "   Val loss: 1.22221\n",
      "\n",
      "EPOCH 27\n",
      " Train loss: 1.24016\n",
      "   Val loss: 1.23683\n",
      "\n",
      "EPOCH 28\n",
      " Train loss: 1.22992\n",
      "   Val loss: 1.22123\n",
      "\n",
      "EPOCH 29\n",
      " Train loss: 1.22835\n",
      "   Val loss: 1.25113\n",
      "\n",
      "EPOCH 30\n",
      " Train loss: 1.22255\n",
      "   Val loss: 1.26793\n",
      "\n",
      "EPOCH 31\n",
      " Train loss: 1.21176\n",
      "   Val loss: 1.21492\n",
      "\n",
      "EPOCH 32\n",
      " Train loss: 1.21349\n",
      "   Val loss: 1.18791\n",
      "\n",
      "EPOCH 33\n",
      " Train loss: 1.21587\n",
      "   Val loss: 1.21588\n",
      "\n",
      "EPOCH 34\n",
      " Train loss: 1.21062\n",
      "   Val loss: 1.20007\n",
      "\n",
      "EPOCH 35\n",
      " Train loss: 1.20141\n",
      "   Val loss: 1.25321\n",
      "\n",
      "EPOCH 36\n",
      " Train loss: 1.20132\n",
      "   Val loss: 1.25203\n",
      "\n",
      "EPOCH 37\n",
      " Train loss: 1.19782\n",
      "   Val loss: 1.23606\n",
      "\n",
      "EPOCH 38\n",
      " Train loss: 1.20255\n",
      "   Val loss: 1.22910\n",
      "\n",
      "EPOCH 39\n",
      " Train loss: 1.19702\n",
      "   Val loss: 1.16341\n",
      "\n",
      "EPOCH 40\n",
      " Train loss: 1.19327\n",
      "   Val loss: 1.19773\n",
      "\n",
      "EPOCH 41\n",
      " Train loss: 1.18886\n",
      "   Val loss: 1.21532\n",
      "\n",
      "EPOCH 42\n",
      " Train loss: 1.18927\n",
      "   Val loss: 1.21579\n",
      "\n",
      "EPOCH 43\n",
      " Train loss: 1.18751\n",
      "   Val loss: 1.19923\n",
      "\n",
      "EPOCH 44\n",
      " Train loss: 1.18344\n",
      "   Val loss: 1.17422\n",
      "\n",
      "EPOCH 45\n",
      " Train loss: 1.18561\n",
      "   Val loss: 1.16734\n",
      "\n",
      "EPOCH 46\n",
      " Train loss: 1.18118\n",
      "   Val loss: 1.25999\n",
      "\n",
      "EPOCH 47\n",
      " Train loss: 1.18617\n",
      "   Val loss: 1.21607\n",
      "\n",
      "EPOCH 48\n",
      " Train loss: 1.18469\n",
      "   Val loss: 1.25986\n",
      "\n",
      "EPOCH 49\n",
      " Train loss: 1.18266\n",
      "   Val loss: 1.12847\n",
      "\n",
      "EPOCH 50\n",
      " Train loss: 1.16718\n",
      "   Val loss: 1.19913\n",
      "\n",
      "EPOCH 51\n",
      " Train loss: 1.17040\n",
      "   Val loss: 1.15675\n",
      "\n",
      "EPOCH 52\n",
      " Train loss: 1.17542\n",
      "   Val loss: 1.21791\n",
      "\n",
      "EPOCH 53\n",
      " Train loss: 1.17484\n",
      "   Val loss: 1.14551\n",
      "\n",
      "EPOCH 54\n",
      " Train loss: 1.17237\n",
      "   Val loss: 1.19719\n",
      "\n",
      "EPOCH 55\n",
      " Train loss: 1.16436\n",
      "   Val loss: 1.23906\n",
      "\n",
      "EPOCH 56\n",
      " Train loss: 1.17322\n",
      "   Val loss: 1.16759\n",
      "\n",
      "EPOCH 57\n",
      " Train loss: 1.16324\n",
      "   Val loss: 1.13453\n",
      "\n",
      "EPOCH 58\n",
      " Train loss: 1.15886\n",
      "   Val loss: 1.17643\n",
      "\n",
      "EPOCH 59\n",
      " Train loss: 1.15465\n",
      "   Val loss: 1.21555\n",
      "\n",
      "EPOCH 60\n",
      " Train loss: 1.15926\n",
      "   Val loss: 1.17693\n",
      "\n",
      "EPOCH 61\n",
      " Train loss: 1.15006\n",
      "   Val loss: 1.13541\n",
      "\n",
      "EPOCH 62\n",
      " Train loss: 1.16112\n",
      "   Val loss: 1.19680\n",
      "\n",
      "EPOCH 63\n",
      " Train loss: 1.15206\n",
      "   Val loss: 1.14183\n",
      "\n",
      "EPOCH 64\n",
      " Train loss: 1.15444\n",
      "   Val loss: 1.15934\n",
      "\n",
      "EPOCH 65\n",
      " Train loss: 1.14837\n",
      "   Val loss: 1.17414\n",
      "\n",
      "EPOCH 66\n",
      " Train loss: 1.14679\n",
      "   Val loss: 1.18529\n",
      "\n",
      "EPOCH 67\n",
      " Train loss: 1.14853\n",
      "   Val loss: 1.15562\n",
      "\n",
      "EPOCH 68\n",
      " Train loss: 1.14222\n",
      "   Val loss: 1.14311\n",
      "\n",
      "EPOCH 69\n",
      " Train loss: 1.15331\n",
      "   Val loss: 1.14764\n",
      "\n",
      "EPOCH 70\n",
      " Train loss: 1.14594\n",
      "   Val loss: 1.15375\n",
      "\n",
      "EPOCH 71\n",
      " Train loss: 1.14603\n",
      "   Val loss: 1.16786\n",
      "\n",
      "EPOCH 72\n",
      " Train loss: 1.13806\n",
      "   Val loss: 1.15996\n",
      "\n",
      "EPOCH 73\n",
      " Train loss: 1.14391\n",
      "   Val loss: 1.12441\n",
      "\n",
      "EPOCH 74\n",
      " Train loss: 1.14842\n",
      "   Val loss: 1.15238\n",
      "\n",
      "EPOCH 75\n",
      " Train loss: 1.13403\n",
      "   Val loss: 1.17431\n",
      "\n",
      "EPOCH 76\n",
      " Train loss: 1.13792\n",
      "   Val loss: 1.12713\n",
      "\n",
      "EPOCH 77\n",
      " Train loss: 1.13505\n",
      "   Val loss: 1.16317\n",
      "\n",
      "EPOCH 78\n",
      " Train loss: 1.13256\n",
      "   Val loss: 1.17194\n",
      "\n",
      "EPOCH 79\n",
      " Train loss: 1.13450\n",
      "   Val loss: 1.19955\n",
      "\n",
      "EPOCH 80\n",
      " Train loss: 1.13542\n",
      "   Val loss: 1.14966\n",
      "\n",
      "EPOCH 81\n",
      " Train loss: 1.13641\n",
      "   Val loss: 1.18960\n",
      "\n",
      "EPOCH 82\n",
      " Train loss: 1.13537\n",
      "   Val loss: 1.13583\n",
      "\n",
      "EPOCH 83\n",
      " Train loss: 1.12900\n",
      "   Val loss: 1.15520\n",
      "\n",
      "EPOCH 84\n",
      " Train loss: 1.12958\n",
      "   Val loss: 1.13244\n",
      "\n",
      "EPOCH 85\n",
      " Train loss: 1.13787\n",
      "   Val loss: 1.15555\n",
      "\n",
      "EPOCH 86\n",
      " Train loss: 1.12041\n",
      "   Val loss: 1.15524\n",
      "\n",
      "EPOCH 87\n",
      " Train loss: 1.12515\n",
      "   Val loss: 1.15477\n",
      "\n",
      "EPOCH 88\n",
      " Train loss: 1.12430\n",
      "   Val loss: 1.14976\n",
      "\n",
      "EPOCH 89\n",
      " Train loss: 1.12539\n",
      "   Val loss: 1.11314\n",
      "\n",
      "EPOCH 90\n",
      " Train loss: 1.12266\n",
      "   Val loss: 1.14125\n",
      "\n",
      "EPOCH 91\n",
      " Train loss: 1.12258\n",
      "   Val loss: 1.14099\n",
      "\n",
      "EPOCH 92\n",
      " Train loss: 1.12450\n",
      "   Val loss: 1.11246\n",
      "\n",
      "EPOCH 93\n",
      " Train loss: 1.12484\n",
      "   Val loss: 1.16705\n",
      "\n",
      "EPOCH 94\n",
      " Train loss: 1.12365\n",
      "   Val loss: 1.17412\n",
      "\n",
      "EPOCH 95\n",
      " Train loss: 1.11862\n",
      "   Val loss: 1.14940\n",
      "\n",
      "EPOCH 96\n",
      " Train loss: 1.12176\n",
      "   Val loss: 1.11312\n",
      "\n",
      "EPOCH 97\n",
      " Train loss: 1.12296\n",
      "   Val loss: 1.12144\n",
      "\n",
      "EPOCH 98\n",
      " Train loss: 1.11619\n",
      "   Val loss: 1.14080\n",
      "\n",
      "EPOCH 99\n",
      " Train loss: 1.12304\n",
      "   Val loss: 1.14865\n",
      "\n",
      "Combined Model accuracy: 53.231\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cba78c5e7348dcaf34cb58d1f33f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      " Train loss: 9.58724\n",
      "   Val loss: 7.05852\n",
      "\n",
      "EPOCH 1\n",
      " Train loss: 5.52440\n",
      "   Val loss: 4.11414\n",
      "\n",
      "EPOCH 2\n",
      " Train loss: 3.40280\n",
      "   Val loss: 2.65779\n",
      "\n",
      "EPOCH 3\n",
      " Train loss: 2.26251\n",
      "   Val loss: 1.88995\n",
      "\n",
      "EPOCH 4\n",
      " Train loss: 1.66877\n",
      "   Val loss: 1.49958\n",
      "\n",
      "EPOCH 5\n",
      " Train loss: 1.43899\n",
      "   Val loss: 1.42410\n",
      "\n",
      "EPOCH 6\n",
      " Train loss: 1.38683\n",
      "   Val loss: 1.45508\n",
      "\n",
      "EPOCH 7\n",
      " Train loss: 1.34084\n",
      "   Val loss: 1.41675\n",
      "\n",
      "EPOCH 8\n",
      " Train loss: 1.32461\n",
      "   Val loss: 1.34208\n",
      "\n",
      "EPOCH 9\n",
      " Train loss: 1.31467\n",
      "   Val loss: 1.37092\n",
      "\n",
      "EPOCH 10\n",
      " Train loss: 1.28189\n",
      "   Val loss: 1.31587\n",
      "\n",
      "EPOCH 11\n",
      " Train loss: 1.28602\n",
      "   Val loss: 1.32465\n",
      "\n",
      "EPOCH 12\n",
      " Train loss: 1.29119\n",
      "   Val loss: 1.35254\n",
      "\n",
      "EPOCH 13\n",
      " Train loss: 1.27810\n",
      "   Val loss: 1.27902\n",
      "\n",
      "EPOCH 14\n",
      " Train loss: 1.27376\n",
      "   Val loss: 1.34762\n",
      "\n",
      "EPOCH 15\n",
      " Train loss: 1.27572\n",
      "   Val loss: 1.26647\n",
      "\n",
      "EPOCH 16\n",
      " Train loss: 1.25634\n",
      "   Val loss: 1.33309\n",
      "\n",
      "EPOCH 17\n",
      " Train loss: 1.26837\n",
      "   Val loss: 1.33992\n",
      "\n",
      "EPOCH 18\n",
      " Train loss: 1.25904\n",
      "   Val loss: 1.29679\n",
      "\n",
      "EPOCH 19\n",
      " Train loss: 1.25883\n",
      "   Val loss: 1.28136\n",
      "\n",
      "EPOCH 20\n",
      " Train loss: 1.26037\n",
      "   Val loss: 1.30639\n",
      "\n",
      "EPOCH 21\n",
      " Train loss: 1.26001\n",
      "   Val loss: 1.29343\n",
      "\n",
      "EPOCH 22\n",
      " Train loss: 1.25782\n",
      "   Val loss: 1.27229\n",
      "\n",
      "EPOCH 23\n",
      " Train loss: 1.25517\n",
      "   Val loss: 1.28007\n",
      "\n",
      "EPOCH 24\n",
      " Train loss: 1.24520\n",
      "   Val loss: 1.24174\n",
      "\n",
      "EPOCH 25\n",
      " Train loss: 1.23862\n",
      "   Val loss: 1.28234\n",
      "\n",
      "EPOCH 26\n",
      " Train loss: 1.23644\n",
      "   Val loss: 1.29534\n",
      "\n",
      "EPOCH 27\n",
      " Train loss: 1.24126\n",
      "   Val loss: 1.26658\n",
      "\n",
      "EPOCH 28\n",
      " Train loss: 1.24657\n",
      "   Val loss: 1.24833\n",
      "\n",
      "EPOCH 29\n",
      " Train loss: 1.23569\n",
      "   Val loss: 1.23962\n",
      "\n",
      "EPOCH 30\n",
      " Train loss: 1.23507\n",
      "   Val loss: 1.26013\n",
      "\n",
      "EPOCH 31\n",
      " Train loss: 1.24609\n",
      "   Val loss: 1.26685\n",
      "\n",
      "EPOCH 32\n",
      " Train loss: 1.23425\n",
      "   Val loss: 1.27510\n",
      "\n",
      "EPOCH 33\n",
      " Train loss: 1.23738\n",
      "   Val loss: 1.25625\n",
      "\n",
      "EPOCH 34\n",
      " Train loss: 1.24743\n",
      "   Val loss: 1.25288\n",
      "\n",
      "EPOCH 35\n",
      " Train loss: 1.22693\n",
      "   Val loss: 1.26025\n",
      "\n",
      "EPOCH 36\n",
      " Train loss: 1.22225\n",
      "   Val loss: 1.23319\n",
      "\n",
      "EPOCH 37\n",
      " Train loss: 1.22214\n",
      "   Val loss: 1.26344\n",
      "\n",
      "EPOCH 38\n",
      " Train loss: 1.22506\n",
      "   Val loss: 1.23382\n",
      "\n",
      "EPOCH 39\n",
      " Train loss: 1.22572\n",
      "   Val loss: 1.23044\n",
      "\n",
      "EPOCH 40\n",
      " Train loss: 1.21921\n",
      "   Val loss: 1.23546\n",
      "\n",
      "EPOCH 41\n",
      " Train loss: 1.21950\n",
      "   Val loss: 1.23131\n",
      "\n",
      "EPOCH 42\n",
      " Train loss: 1.21708\n",
      "   Val loss: 1.23189\n",
      "\n",
      "EPOCH 43\n",
      " Train loss: 1.21324\n",
      "   Val loss: 1.24977\n",
      "\n",
      "EPOCH 44\n",
      " Train loss: 1.22754\n",
      "   Val loss: 1.24660\n",
      "\n",
      "EPOCH 45\n",
      " Train loss: 1.21218\n",
      "   Val loss: 1.22748\n",
      "\n",
      "EPOCH 46\n",
      " Train loss: 1.21690\n",
      "   Val loss: 1.21893\n",
      "\n",
      "EPOCH 47\n",
      " Train loss: 1.22200\n",
      "   Val loss: 1.22904\n",
      "\n",
      "EPOCH 48\n",
      " Train loss: 1.21937\n",
      "   Val loss: 1.22341\n",
      "\n",
      "EPOCH 49\n",
      " Train loss: 1.21162\n",
      "   Val loss: 1.19571\n",
      "\n",
      "EPOCH 50\n",
      " Train loss: 1.21772\n",
      "   Val loss: 1.24983\n",
      "\n",
      "EPOCH 51\n",
      " Train loss: 1.21019\n",
      "   Val loss: 1.22532\n",
      "\n",
      "EPOCH 52\n",
      " Train loss: 1.20683\n",
      "   Val loss: 1.22764\n",
      "\n",
      "EPOCH 53\n",
      " Train loss: 1.22183\n",
      "   Val loss: 1.23587\n",
      "\n",
      "EPOCH 54\n",
      " Train loss: 1.20450\n",
      "   Val loss: 1.23855\n",
      "\n",
      "EPOCH 55\n",
      " Train loss: 1.20537\n",
      "   Val loss: 1.18071\n",
      "\n",
      "EPOCH 56\n",
      " Train loss: 1.20684\n",
      "   Val loss: 1.23467\n",
      "\n",
      "EPOCH 57\n",
      " Train loss: 1.20886\n",
      "   Val loss: 1.23438\n",
      "\n",
      "EPOCH 58\n",
      " Train loss: 1.20622\n",
      "   Val loss: 1.19539\n",
      "\n",
      "EPOCH 59\n",
      " Train loss: 1.20124\n",
      "   Val loss: 1.19711\n",
      "\n",
      "EPOCH 60\n",
      " Train loss: 1.19598\n",
      "   Val loss: 1.20770\n",
      "\n",
      "EPOCH 61\n",
      " Train loss: 1.20133\n",
      "   Val loss: 1.20631\n",
      "\n",
      "EPOCH 62\n",
      " Train loss: 1.20432\n",
      "   Val loss: 1.19215\n",
      "\n",
      "EPOCH 63\n",
      " Train loss: 1.20000\n",
      "   Val loss: 1.22233\n",
      "\n",
      "EPOCH 64\n",
      " Train loss: 1.19170\n",
      "   Val loss: 1.20022\n",
      "\n",
      "EPOCH 65\n",
      " Train loss: 1.19366\n",
      "   Val loss: 1.24633\n",
      "\n",
      "EPOCH 66\n",
      " Train loss: 1.18256\n",
      "   Val loss: 1.19937\n",
      "\n",
      "EPOCH 67\n",
      " Train loss: 1.19687\n",
      "   Val loss: 1.21076\n",
      "\n",
      "EPOCH 68\n",
      " Train loss: 1.19004\n",
      "   Val loss: 1.20849\n",
      "\n",
      "EPOCH 69\n",
      " Train loss: 1.18803\n",
      "   Val loss: 1.20591\n",
      "\n",
      "EPOCH 70\n",
      " Train loss: 1.18292\n",
      "   Val loss: 1.20445\n",
      "\n",
      "EPOCH 71\n",
      " Train loss: 1.18574\n",
      "   Val loss: 1.15876\n",
      "\n",
      "EPOCH 72\n",
      " Train loss: 1.18571\n",
      "   Val loss: 1.17925\n",
      "\n",
      "EPOCH 73\n",
      " Train loss: 1.19769\n",
      "   Val loss: 1.20171\n",
      "\n",
      "EPOCH 74\n",
      " Train loss: 1.18276\n",
      "   Val loss: 1.19141\n",
      "\n",
      "EPOCH 75\n",
      " Train loss: 1.19347\n",
      "   Val loss: 1.20324\n",
      "\n",
      "EPOCH 76\n",
      " Train loss: 1.17939\n",
      "   Val loss: 1.22434\n",
      "\n",
      "EPOCH 77\n",
      " Train loss: 1.17980\n",
      "   Val loss: 1.21763\n",
      "\n",
      "EPOCH 78\n",
      " Train loss: 1.18267\n",
      "   Val loss: 1.18918\n",
      "\n",
      "EPOCH 79\n",
      " Train loss: 1.18167\n",
      "   Val loss: 1.19508\n",
      "\n",
      "EPOCH 80\n",
      " Train loss: 1.18098\n",
      "   Val loss: 1.20034\n",
      "\n",
      "EPOCH 81\n",
      " Train loss: 1.18706\n",
      "   Val loss: 1.23723\n",
      "\n",
      "EPOCH 82\n",
      " Train loss: 1.18240\n",
      "   Val loss: 1.16726\n",
      "\n",
      "EPOCH 83\n",
      " Train loss: 1.17998\n",
      "   Val loss: 1.18298\n",
      "\n",
      "EPOCH 84\n",
      " Train loss: 1.16746\n",
      "   Val loss: 1.18941\n",
      "\n",
      "EPOCH 85\n",
      " Train loss: 1.18618\n",
      "   Val loss: 1.18283\n",
      "\n",
      "EPOCH 86\n",
      " Train loss: 1.17301\n",
      "   Val loss: 1.18821\n",
      "\n",
      "EPOCH 87\n",
      " Train loss: 1.17265\n",
      "   Val loss: 1.17446\n",
      "\n",
      "EPOCH 88\n",
      " Train loss: 1.16078\n",
      "   Val loss: 1.16410\n",
      "\n",
      "EPOCH 89\n",
      " Train loss: 1.17359\n",
      "   Val loss: 1.17157\n",
      "\n",
      "EPOCH 90\n",
      " Train loss: 1.17249\n",
      "   Val loss: 1.18918\n",
      "\n",
      "EPOCH 91\n",
      " Train loss: 1.17601\n",
      "   Val loss: 1.18499\n",
      "\n",
      "EPOCH 92\n",
      " Train loss: 1.16117\n",
      "   Val loss: 1.17403\n",
      "\n",
      "EPOCH 93\n",
      " Train loss: 1.16672\n",
      "   Val loss: 1.17071\n",
      "\n",
      "EPOCH 94\n",
      " Train loss: 1.16722\n",
      "   Val loss: 1.18376\n",
      "\n",
      "EPOCH 95\n",
      " Train loss: 1.16944\n",
      "   Val loss: 1.18968\n",
      "\n",
      "EPOCH 96\n",
      " Train loss: 1.17958\n",
      "   Val loss: 1.16891\n",
      "\n",
      "EPOCH 97\n",
      " Train loss: 1.17261\n",
      "   Val loss: 1.18150\n",
      "\n",
      "EPOCH 98\n",
      " Train loss: 1.18179\n",
      "   Val loss: 1.16993\n",
      "\n",
      "EPOCH 99\n",
      " Train loss: 1.16965\n",
      "   Val loss: 1.18668\n",
      "\n",
      "White Model accuracy: 48.265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496449ec0c6147508a7989bd2df4fa54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      " Train loss: 7.19860\n",
      "   Val loss: 4.35149\n",
      "\n",
      "EPOCH 1\n",
      " Train loss: 3.58519\n",
      "   Val loss: 2.58308\n",
      "\n",
      "EPOCH 2\n",
      " Train loss: 2.83806\n",
      "   Val loss: 2.45503\n",
      "\n",
      "EPOCH 3\n",
      " Train loss: 2.48463\n",
      "   Val loss: 2.43819\n",
      "\n",
      "EPOCH 4\n",
      " Train loss: 2.24376\n",
      "   Val loss: 1.86303\n",
      "\n",
      "EPOCH 5\n",
      " Train loss: 1.93680\n",
      "   Val loss: 2.31657\n",
      "\n",
      "EPOCH 6\n",
      " Train loss: 1.87584\n",
      "   Val loss: 1.77567\n",
      "\n",
      "EPOCH 7\n",
      " Train loss: 1.72627\n",
      "   Val loss: 1.85507\n",
      "\n",
      "EPOCH 8\n",
      " Train loss: 1.55620\n",
      "   Val loss: 1.75324\n",
      "\n",
      "EPOCH 9\n",
      " Train loss: 1.58548\n",
      "   Val loss: 1.35582\n",
      "\n",
      "EPOCH 10\n",
      " Train loss: 1.48428\n",
      "   Val loss: 1.35905\n",
      "\n",
      "EPOCH 11\n",
      " Train loss: 1.41701\n",
      "   Val loss: 1.18005\n",
      "\n",
      "EPOCH 12\n",
      " Train loss: 1.36052\n",
      "   Val loss: 1.22191\n",
      "\n",
      "EPOCH 13\n",
      " Train loss: 1.29172\n",
      "   Val loss: 1.43618\n",
      "\n",
      "EPOCH 14\n",
      " Train loss: 1.28235\n",
      "   Val loss: 1.54188\n",
      "\n",
      "EPOCH 15\n",
      " Train loss: 1.27234\n",
      "   Val loss: 1.24457\n",
      "\n",
      "EPOCH 16\n",
      " Train loss: 1.25984\n",
      "   Val loss: 1.24183\n",
      "\n",
      "EPOCH 17\n",
      " Train loss: 1.23643\n",
      "   Val loss: 1.26077\n",
      "\n",
      "EPOCH 18\n",
      " Train loss: 1.25079\n",
      "   Val loss: 1.25868\n",
      "\n",
      "EPOCH 19\n",
      " Train loss: 1.20333\n",
      "   Val loss: 1.20667\n",
      "\n",
      "EPOCH 20\n",
      " Train loss: 1.17378\n",
      "   Val loss: 1.16110\n",
      "\n",
      "EPOCH 21\n",
      " Train loss: 1.19485\n",
      "   Val loss: 1.34847\n",
      "\n",
      "EPOCH 22\n",
      " Train loss: 1.20903\n",
      "   Val loss: 1.23428\n",
      "\n",
      "EPOCH 23\n",
      " Train loss: 1.21335\n",
      "   Val loss: 1.21249\n",
      "\n",
      "EPOCH 24\n",
      " Train loss: 1.17654\n",
      "   Val loss: 1.19589\n",
      "\n",
      "EPOCH 25\n",
      " Train loss: 1.16984\n",
      "   Val loss: 1.10182\n",
      "\n",
      "EPOCH 26\n",
      " Train loss: 1.15970\n",
      "   Val loss: 1.16485\n",
      "\n",
      "EPOCH 27\n",
      " Train loss: 1.17486\n",
      "   Val loss: 1.14255\n",
      "\n",
      "EPOCH 28\n",
      " Train loss: 1.16251\n",
      "   Val loss: 1.30385\n",
      "\n",
      "EPOCH 29\n",
      " Train loss: 1.17065\n",
      "   Val loss: 1.17580\n",
      "\n",
      "EPOCH 30\n",
      " Train loss: 1.16243\n",
      "   Val loss: 1.31922\n",
      "\n",
      "EPOCH 31\n",
      " Train loss: 1.17515\n",
      "   Val loss: 1.09641\n",
      "\n",
      "EPOCH 32\n",
      " Train loss: 1.16435\n",
      "   Val loss: 1.10002\n",
      "\n",
      "EPOCH 33\n",
      " Train loss: 1.16374\n",
      "   Val loss: 1.07186\n",
      "\n",
      "EPOCH 34\n",
      " Train loss: 1.13412\n",
      "   Val loss: 1.13346\n",
      "\n",
      "EPOCH 35\n",
      " Train loss: 1.15301\n",
      "   Val loss: 1.01788\n",
      "\n",
      "EPOCH 36\n",
      " Train loss: 1.15332\n",
      "   Val loss: 1.12029\n",
      "\n",
      "EPOCH 37\n",
      " Train loss: 1.13504\n",
      "   Val loss: 1.19543\n",
      "\n",
      "EPOCH 38\n",
      " Train loss: 1.13951\n",
      "   Val loss: 1.16667\n",
      "\n",
      "EPOCH 39\n",
      " Train loss: 1.14005\n",
      "   Val loss: 1.13737\n",
      "\n",
      "EPOCH 40\n",
      " Train loss: 1.14022\n",
      "   Val loss: 1.12697\n",
      "\n",
      "EPOCH 41\n",
      " Train loss: 1.13068\n",
      "   Val loss: 1.17394\n",
      "\n",
      "EPOCH 42\n",
      " Train loss: 1.14210\n",
      "   Val loss: 1.17840\n",
      "\n",
      "EPOCH 43\n",
      " Train loss: 1.13381\n",
      "   Val loss: 1.15179\n",
      "\n",
      "EPOCH 44\n",
      " Train loss: 1.15816\n",
      "   Val loss: 1.21586\n",
      "\n",
      "EPOCH 45\n",
      " Train loss: 1.12817\n",
      "   Val loss: 1.27738\n",
      "\n",
      "EPOCH 46\n",
      " Train loss: 1.13616\n",
      "   Val loss: 1.12943\n",
      "\n",
      "EPOCH 47\n",
      " Train loss: 1.14020\n",
      "   Val loss: 1.04909\n",
      "\n",
      "EPOCH 48\n",
      " Train loss: 1.13438\n",
      "   Val loss: 1.16165\n",
      "\n",
      "EPOCH 49\n",
      " Train loss: 1.12533\n",
      "   Val loss: 1.19730\n",
      "\n",
      "EPOCH 50\n",
      " Train loss: 1.12777\n",
      "   Val loss: 1.12678\n",
      "\n",
      "EPOCH 51\n",
      " Train loss: 1.11661\n",
      "   Val loss: 1.13301\n",
      "\n",
      "EPOCH 52\n",
      " Train loss: 1.13038\n",
      "   Val loss: 1.08184\n",
      "\n",
      "EPOCH 53\n",
      " Train loss: 1.14275\n",
      "   Val loss: 1.12530\n",
      "\n",
      "EPOCH 54\n",
      " Train loss: 1.14325\n",
      "   Val loss: 1.23084\n",
      "\n",
      "EPOCH 55\n",
      " Train loss: 1.12481\n",
      "   Val loss: 1.15832\n",
      "\n",
      "EPOCH 56\n",
      " Train loss: 1.11252\n",
      "   Val loss: 1.19205\n",
      "\n",
      "EPOCH 57\n",
      " Train loss: 1.14146\n",
      "   Val loss: 1.26756\n",
      "\n",
      "EPOCH 58\n",
      " Train loss: 1.13349\n",
      "   Val loss: 1.22832\n",
      "\n",
      "EPOCH 59\n",
      " Train loss: 1.13484\n",
      "   Val loss: 1.22406\n",
      "\n",
      "EPOCH 60\n",
      " Train loss: 1.12317\n",
      "   Val loss: 1.17250\n",
      "\n",
      "EPOCH 61\n",
      " Train loss: 1.11474\n",
      "   Val loss: 1.04700\n",
      "\n",
      "EPOCH 62\n",
      " Train loss: 1.11333\n",
      "   Val loss: 1.18299\n",
      "\n",
      "EPOCH 63\n",
      " Train loss: 1.11195\n",
      "   Val loss: 1.04653\n",
      "\n",
      "EPOCH 64\n",
      " Train loss: 1.12061\n",
      "   Val loss: 1.05695\n",
      "\n",
      "EPOCH 65\n",
      " Train loss: 1.11394\n",
      "   Val loss: 1.19427\n",
      "\n",
      "EPOCH 66\n",
      " Train loss: 1.10648\n",
      "   Val loss: 1.06608\n",
      "\n",
      "EPOCH 67\n",
      " Train loss: 1.12986\n",
      "   Val loss: 1.23877\n",
      "\n",
      "EPOCH 68\n",
      " Train loss: 1.11321\n",
      "   Val loss: 1.17579\n",
      "\n",
      "EPOCH 69\n",
      " Train loss: 1.13344\n",
      "   Val loss: 1.03487\n",
      "\n",
      "EPOCH 70\n",
      " Train loss: 1.11214\n",
      "   Val loss: 1.05441\n",
      "\n",
      "EPOCH 71\n",
      " Train loss: 1.11206\n",
      "   Val loss: 1.07298\n",
      "\n",
      "EPOCH 72\n",
      " Train loss: 1.12533\n",
      "   Val loss: 1.09654\n",
      "\n",
      "EPOCH 73\n",
      " Train loss: 1.11619\n",
      "   Val loss: 1.12347\n",
      "\n",
      "EPOCH 74\n",
      " Train loss: 1.10933\n",
      "   Val loss: 1.16086\n",
      "\n",
      "EPOCH 75\n",
      " Train loss: 1.10291\n",
      "   Val loss: 1.25728\n",
      "\n",
      "EPOCH 76\n",
      " Train loss: 1.10153\n",
      "   Val loss: 1.12097\n",
      "\n",
      "EPOCH 77\n",
      " Train loss: 1.11472\n",
      "   Val loss: 1.03691\n",
      "\n",
      "EPOCH 78\n",
      " Train loss: 1.11212\n",
      "   Val loss: 1.04278\n",
      "\n",
      "EPOCH 79\n",
      " Train loss: 1.09853\n",
      "   Val loss: 1.08246\n",
      "\n",
      "EPOCH 80\n",
      " Train loss: 1.11108\n",
      "   Val loss: 1.01084\n",
      "\n",
      "EPOCH 81\n",
      " Train loss: 1.10036\n",
      "   Val loss: 1.05283\n",
      "\n",
      "EPOCH 82\n",
      " Train loss: 1.09690\n",
      "   Val loss: 1.13403\n",
      "\n",
      "EPOCH 83\n",
      " Train loss: 1.09081\n",
      "   Val loss: 1.17452\n",
      "\n",
      "EPOCH 84\n",
      " Train loss: 1.09610\n",
      "   Val loss: 1.16875\n",
      "\n",
      "EPOCH 85\n",
      " Train loss: 1.11016\n",
      "   Val loss: 1.09844\n",
      "\n",
      "EPOCH 86\n",
      " Train loss: 1.09062\n",
      "   Val loss: 1.23879\n",
      "\n",
      "EPOCH 87\n",
      " Train loss: 1.10146\n",
      "   Val loss: 1.09889\n",
      "\n",
      "EPOCH 88\n",
      " Train loss: 1.08729\n",
      "   Val loss: 1.11994\n",
      "\n",
      "EPOCH 89\n",
      " Train loss: 1.09489\n",
      "   Val loss: 1.18351\n",
      "\n",
      "EPOCH 90\n",
      " Train loss: 1.10128\n",
      "   Val loss: 1.07574\n",
      "\n",
      "EPOCH 91\n",
      " Train loss: 1.11514\n",
      "   Val loss: 1.05470\n",
      "\n",
      "EPOCH 92\n",
      " Train loss: 1.11593\n",
      "   Val loss: 1.04643\n",
      "\n",
      "EPOCH 93\n",
      " Train loss: 1.08681\n",
      "   Val loss: 1.19194\n",
      "\n",
      "EPOCH 94\n",
      " Train loss: 1.08599\n",
      "   Val loss: 1.06172\n",
      "\n",
      "EPOCH 95\n",
      " Train loss: 1.08248\n",
      "   Val loss: 1.25077\n",
      "\n",
      "EPOCH 96\n",
      " Train loss: 1.07223\n",
      "   Val loss: 1.06893\n",
      "\n",
      "EPOCH 97\n",
      " Train loss: 1.09541\n",
      "   Val loss: 1.28486\n",
      "\n",
      "EPOCH 98\n",
      " Train loss: 1.06514\n",
      "   Val loss: 1.08728\n",
      "\n",
      "EPOCH 99\n",
      " Train loss: 1.08290\n",
      "   Val loss: 1.10418\n",
      "\n",
      "Red Model accuracy: 52.812\n"
     ]
    }
   ],
   "source": [
    "model = WineQualityModel((\n",
    "    torch.nn.Linear(11, 81),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(81, 11)\n",
    "))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_and_test(model, train_loader, val_loader, test_loader, criterion, optimizer, \"Combined Model\")\n",
    "\n",
    "model_white = WineQualityModel((\n",
    "    torch.nn.Linear(11, 81),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(81, 11)\n",
    "))\n",
    "\n",
    "optimizer_white = torch.optim.Adam(model_white.parameters(), lr=1e-3)\n",
    "train_and_test(model_white, train_loader_white, val_loader_white, test_loader_white, criterion, optimizer_white, \"White Model\")\n",
    "\n",
    "model_red = WineQualityModel((\n",
    "    torch.nn.Linear(11, 81),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(81, 11)\n",
    "))\n",
    "\n",
    "optimizer_red = torch.optim.Adam(model_red.parameters(), lr=1e-3)\n",
    "train_and_test(model_red, train_loader_red, val_loader_red, test_loader_red, criterion, optimizer_red, \"Red Model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b295cf7",
   "metadata": {},
   "source": [
    "## Hypertuning\n",
    "\n",
    "Now that we have the ability to initialize and train a model, we're going to try hypertuning some of the parameters. Specifically we want to try tuning the number of layers and the hidden size of each of those layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14b4ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_report_model(layers):\n",
    "    loaders = [(train_loader, val_loader, test_loader), \\\n",
    "               (train_loader_white, val_loader_white, test_loader_white), \\\n",
    "               (train_loader_red, val_loader_red, test_loader_red)]\n",
    "    #models = []\n",
    "    #accuracies = []\n",
    "    true_lists = []\n",
    "    pred_lists = []\n",
    "    for i in range(3):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        cur_loaders = loaders[i]\n",
    "        cur_model = WineQualityModel(layers)\n",
    "        optimizer = torch.optim.Adam(cur_model.parameters(), lr=1e-3)\n",
    "        acc, true, pred = train_and_test(cur_model, *cur_loaders, criterion, optimizer, f\"Model {i}\", 200)\n",
    "        true_lists.append(true)\n",
    "        pred_lists.append(pred)\n",
    "    return true_lists, pred_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f14b110f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum_layers = 1\\nhidden_sizes = range()\\n\\nfor i in range(num_layers):\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "num_layers = 1\n",
    "hidden_sizes = range()\n",
    "\n",
    "for i in range(num_layers):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4257b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
